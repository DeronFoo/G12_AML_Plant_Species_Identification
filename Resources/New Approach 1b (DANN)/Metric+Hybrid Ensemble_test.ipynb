{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ca28ae7",
   "metadata": {},
   "source": [
    "# Import Libraries and Setup Constant Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8e7c406",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\William\\School\\Swinburne\\Computer Science\\2025 Semester 2\\COS30082 Applied Machine Learning\\Assignment 2\\Approach3\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Project Root: c:\\Users\\William\\School\\Swinburne\\Computer Science\\2025 Semester 2\\COS30082 Applied Machine Learning\\Assignment 2\\Approach3\n",
      "Data Root: c:\\Users\\William\\School\\Swinburne\\Computer Science\\2025 Semester 2\\COS30082 Applied Machine Learning\\Assignment 2\\Approach3\\AML_project_herbarium_dataset\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Tuple\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import importlib\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.cuda import amp\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "import timm\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Pass configurations\n",
    "# =========================\n",
    "PROJECT_ROOT = Path.cwd().parents[1]\n",
    "\n",
    "DATASET_DIRNAME = \"AML_project_herbarium_dataset\"\n",
    "DATA_ROOT = PROJECT_ROOT / DATASET_DIRNAME\n",
    "\n",
    "TRAIN_DIR = DATA_ROOT / \"train\"          \n",
    "TEST_DIR = DATA_ROOT / \"test\"              \n",
    "\n",
    "LIST_DIR = DATA_ROOT / \"list\"\n",
    "TRAIN_LIST = LIST_DIR / \"train.txt\"\n",
    "TEST_LIST = LIST_DIR / \"test.txt\"\n",
    "SPECIES_LIST = LIST_DIR / \"species_list.txt\"\n",
    "GROUNDTRUTH = LIST_DIR / \"groundtruth.txt\"\n",
    "CLASS_WITH_PAIRS = LIST_DIR / \"class_with_pairs.txt\"\n",
    "CLASS_WITHOUT_PAIRS = LIST_DIR / \"class_without_pairs.txt\"\n",
    "\n",
    "NUM_CLASSES = 100\n",
    "\n",
    "IMAGE_SIZE = 518\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "# =========================\n",
    "# Device Setup\n",
    "# =========================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Project Root: {PROJECT_ROOT}\")\n",
    "print(f\"Data Root: {DATA_ROOT}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Constant Configuration \n",
    "# =========================\n",
    "HERBARIUM_DOMAIN = 0  # 0 = herbarium, 1 = photo\n",
    "PHOTO_DOMAIN = 1\n",
    "EMBED_DIM = 512\n",
    "NUM_WORKERS = 0\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 8\n",
    "USE_AMP = (device.type == \"cuda\")\n",
    "\n",
    "LR_BACKBONE_MAX = 1e-6  # Topmost backbone layers\n",
    "LAYER_DECAY = 0.8       # Each layer gets 80% of the LR of the layer above it\n",
    "LR_BACKBONE = 1e-6      # smaller LR for pretrained DINO\n",
    "LR_HEAD = 1e-6          # larger LR for randomly init projection head\n",
    "WEIGHT_DECAY = 1e-4\n",
    "TRIPLET_BATCH_SIZE = 4\n",
    "TRIPLET_NUM_WORKERS = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbd7700",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447a07cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing datasets...\n",
      "Train size: 4744\n",
      "Test size: 207\n",
      "With-Pairs classes: 60\n",
      "Without-Pairs classes: 40\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 2. Data Preprocessing\n",
    "# =========================\n",
    "\n",
    "# --- Transforms ---\n",
    "\n",
    "def build_train_transform(image_size: int = IMAGE_SIZE):\n",
    "    return transforms.Compose([\n",
    "        transforms.RandomResizedCrop(image_size, scale=(0.8, 1.0)),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.ColorJitter(\n",
    "            brightness=0.4,\n",
    "            contrast=0.4,\n",
    "            saturation=0.2,\n",
    "            hue=0.1,\n",
    "        ),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "    ])\n",
    "\n",
    "def build_eval_transform(image_size: int = IMAGE_SIZE):\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "    ])\n",
    "\n",
    "# --- Data Utilities ---\n",
    "\n",
    "_LABEL_MAP = None\n",
    "\n",
    "def _get_label_map():\n",
    "    global _LABEL_MAP\n",
    "    if _LABEL_MAP is not None:\n",
    "        return _LABEL_MAP\n",
    "\n",
    "    mapping = {}\n",
    "    idx = 0\n",
    "    if not SPECIES_LIST.exists():\n",
    "        print(f\"Warning: {SPECIES_LIST} not found.\")\n",
    "        return {}\n",
    "        \n",
    "    with SPECIES_LIST.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            left = line.split(\";\", 1)[0].strip()\n",
    "            try:\n",
    "                raw_id = int(left)\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "            if raw_id not in mapping:\n",
    "                mapping[raw_id] = idx\n",
    "                idx += 1\n",
    "\n",
    "    _LABEL_MAP = mapping\n",
    "    return mapping\n",
    "\n",
    "def _map_label(raw_label: int) -> int:\n",
    "    mapping = _get_label_map()\n",
    "    if raw_label not in mapping:\n",
    "        # Fallback or error based on preference\n",
    "        raise KeyError(f\"Raw label {raw_label} not found in species_list.txt\")\n",
    "    return mapping[raw_label]\n",
    "\n",
    "def _load_raw_id_list(path: Path):\n",
    "    ids = []\n",
    "    if not path.exists():\n",
    "        return ids\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            token = line.split(\";\", 1)[0].split()[0]\n",
    "            try:\n",
    "                raw_id = int(token)\n",
    "            except ValueError:\n",
    "                continue\n",
    "            ids.append(raw_id)\n",
    "    return ids\n",
    "\n",
    "def get_with_without_label_sets():\n",
    "    mapping = _get_label_map()\n",
    "    with_ids = _load_raw_id_list(CLASS_WITH_PAIRS)\n",
    "    without_ids = _load_raw_id_list(CLASS_WITHOUT_PAIRS)\n",
    "\n",
    "    with_set = {mapping[i] for i in with_ids if i in mapping}\n",
    "    without_set = {mapping[i] for i in without_ids if i in mapping}\n",
    "    return with_set, without_set\n",
    "\n",
    "def _parse_train_list(path: Path, root: Path) -> List[Dict]:\n",
    "    samples: List[Dict] = []\n",
    "    if not path.exists():\n",
    "        print(f\"Error: Train list {path} not found.\")\n",
    "        return samples\n",
    "        \n",
    "    with path.open(\"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            rel_path, label_str = line.split()\n",
    "            label = _map_label(int(label_str))\n",
    "            full_path = root / rel_path\n",
    "\n",
    "            if \"herbarium\" in rel_path:\n",
    "                domain = 0\n",
    "            elif \"photo\" in rel_path:\n",
    "                domain = 1\n",
    "            else:\n",
    "                # fallback\n",
    "                domain = 0\n",
    "\n",
    "            samples.append({\n",
    "                \"path\": full_path,\n",
    "                \"label\": label,\n",
    "                \"domain\": domain,\n",
    "                \"rel_path\": rel_path,\n",
    "            })\n",
    "    return samples\n",
    "\n",
    "def _parse_test_list_with_groundtruth(test_list_path: Path, gt_path: Path, dataset_root: Path) -> List[Dict]:\n",
    "    rel_paths: List[str] = []\n",
    "    if not test_list_path.exists() or not gt_path.exists():\n",
    "        print(\"Error: Test list or Groundtruth not found.\")\n",
    "        return []\n",
    "\n",
    "    with test_list_path.open(\"r\") as f:\n",
    "        for line in f:\n",
    "            rel_path = line.strip()\n",
    "            if not rel_path:\n",
    "                continue\n",
    "            rel_paths.append(rel_path)\n",
    "\n",
    "    gt_entries: List[Tuple[str, int]] = []\n",
    "    with gt_path.open(\"r\") as f:\n",
    "        for raw in f:\n",
    "            line = raw.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            parts = line.split()\n",
    "            if len(parts) == 1:\n",
    "                gt_entries.append((\"\", _map_label(int(parts[0]))))\n",
    "            else:\n",
    "                rel_from_gt = parts[0]\n",
    "                label = _map_label(int(parts[-1]))\n",
    "                gt_entries.append((rel_from_gt, label))\n",
    "\n",
    "    if len(rel_paths) != len(gt_entries):\n",
    "        raise ValueError(f\"Mismatch length: {len(rel_paths)} vs {len(gt_entries)}\")\n",
    "\n",
    "    samples: List[Dict] = []\n",
    "    for idx, rel_path in enumerate(rel_paths):\n",
    "        rel_from_gt, label = gt_entries[idx]\n",
    "        if rel_from_gt and rel_from_gt != rel_path:\n",
    "            raise ValueError(f\"Mismatch at line {idx + 1}: '{rel_from_gt}' vs '{rel_path}'\")\n",
    "\n",
    "        full_path = dataset_root / rel_path\n",
    "        samples.append({\n",
    "            \"path\": full_path,\n",
    "            \"label\": label,\n",
    "            \"domain\": 1,\n",
    "            \"rel_path\": rel_path,\n",
    "        })\n",
    "\n",
    "    return samples\n",
    "\n",
    "class HerbFieldDataset(Dataset):\n",
    "    def __init__(self, samples: List[Dict], transform=None):\n",
    "        self.samples = samples\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        s = self.samples[idx]\n",
    "        try:\n",
    "            img = Image.open(s[\"path\"]).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {s['path']}: {e}\")\n",
    "            # Create blank image on failure to prevent crash\n",
    "            img = Image.new(\"RGB\", (IMAGE_SIZE, IMAGE_SIZE))\n",
    "            \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return {\n",
    "            \"image\": img,\n",
    "            \"label\": s[\"label\"],\n",
    "            \"domain\": s[\"domain\"],\n",
    "            \"rel_path\": s[\"rel_path\"],\n",
    "        }\n",
    "\n",
    "def compute_class_weights(samples: List[Dict]) -> torch.Tensor:\n",
    "    counts = torch.zeros(NUM_CLASSES, dtype=torch.float)\n",
    "    for s in samples:\n",
    "        counts[s[\"label\"]] += 1.0\n",
    "    counts = torch.clamp(counts, min=1.0)\n",
    "    weights = 1.0 / torch.log1p(counts)\n",
    "    return weights\n",
    "\n",
    "def build_train_dataset() -> HerbFieldDataset:\n",
    "    samples = _parse_train_list(TRAIN_LIST, DATA_ROOT)\n",
    "    return HerbFieldDataset(samples, transform=build_train_transform())\n",
    "\n",
    "def build_test_dataset() -> HerbFieldDataset:\n",
    "    samples = _parse_test_list_with_groundtruth(TEST_LIST, GROUNDTRUTH, DATA_ROOT)\n",
    "    return HerbFieldDataset(samples, transform=build_eval_transform())\n",
    "\n",
    "# --- Initialization ---\n",
    "\n",
    "print(\"Initializing datasets...\")\n",
    "train_ds = build_train_dataset()\n",
    "test_ds = build_test_dataset()\n",
    "with_set, without_set = get_with_without_label_sets()\n",
    "\n",
    "print(f\"Train size: {len(train_ds)}\")\n",
    "print(f\"Test size: {len(test_ds)}\")\n",
    "print(f\"With-Pairs classes: {len(with_set)}\")\n",
    "print(f\"Without-Pairs classes: {len(without_set)}\")\n",
    "all_train_labels = {s[\"label\"] for s in train_ds.samples}\n",
    "all_test_labels = {s[\"label\"] for s in test_ds.samples}\n",
    "\n",
    "print(\"Distinct train labels:\", len(all_train_labels))\n",
    "print(\"Distinct test labels :\", len(all_test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593af59b",
   "metadata": {},
   "source": [
    "# Dataset Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71d42b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TripletDataset] Usable with-pair classes: 60\n",
      "Triplet batch shapes: torch.Size([4, 3, 518, 518]) torch.Size([4, 3, 518, 518]) torch.Size([4, 3, 518, 518])\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 3. Data Loaders & Triplet Dataset\n",
    "# =========================\n",
    "\n",
    "# --- A. Standard Classification Loaders (For Hybrid Model / Evaluation) ---\n",
    "# We need these for the CrossEntropy loss and for calculating Prototypes.\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "# --- B. Triplet Dataset (Unified Superset) ---\n",
    "# This implementation (from metric_learning.ipynb) computes indices internally\n",
    "# and returns extra metadata (labels/domains), supporting BOTH model types.\n",
    "\n",
    "class TripletDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Samples cross-domain triplets:\n",
    "      - anchor: herbarium OR photo\n",
    "      - positive: same class, other domain\n",
    "      - negative: different class (any domain)\n",
    "    Using only with-pair species (labels in with_set).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_dataset, with_labels):\n",
    "        \"\"\"\n",
    "        base_dataset: HerbFieldDataset (train)\n",
    "        with_labels : iterable of label indices that have herbarium-photo pairs\n",
    "        \"\"\"\n",
    "        self.base_dataset = base_dataset\n",
    "        self.with_labels = sorted(set(with_labels))\n",
    "\n",
    "        # Pre-build index: label -> {0: [idxs of herbarium], 1: [idxs of photo]}\n",
    "        self.label_domain_index = {}\n",
    "        for idx, s in enumerate(self.base_dataset.samples):\n",
    "            lbl = s[\"label\"]\n",
    "            dom = s[\"domain\"]\n",
    "            if lbl not in self.with_labels:\n",
    "                continue\n",
    "            if lbl not in self.label_domain_index:\n",
    "                self.label_domain_index[lbl] = {\n",
    "                    HERBARIUM_DOMAIN: [],\n",
    "                    PHOTO_DOMAIN: [],\n",
    "                }\n",
    "            self.label_domain_index[lbl][dom].append(idx)\n",
    "\n",
    "        # Filter out labels that don't actually have both domains\n",
    "        cleaned_labels = []\n",
    "        for lbl in self.with_labels:\n",
    "            doms = self.label_domain_index.get(lbl, None)\n",
    "            if doms is None:\n",
    "                continue\n",
    "            if len(doms[HERBARIUM_DOMAIN]) > 0 and len(doms[PHOTO_DOMAIN]) > 0:\n",
    "                cleaned_labels.append(lbl)\n",
    "\n",
    "        self.with_labels = cleaned_labels\n",
    "        print(f\"[TripletDataset] Usable with-pair classes: {len(self.with_labels)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        # We can define this arbitrarily since we sample randomly. \n",
    "        # Using base dataset length ensures a \"full\" epoch feel.\n",
    "        return len(self.base_dataset)\n",
    "\n",
    "    def _sample_cross_pair(self):\n",
    "        # Choose a class that has both domains\n",
    "        lbl = random.choice(self.with_labels)\n",
    "        \n",
    "        # Randomly pick which domain is anchor vs positive\n",
    "        dom_anchor = random.choice([HERBARIUM_DOMAIN, PHOTO_DOMAIN])\n",
    "        dom_pos = PHOTO_DOMAIN if dom_anchor == HERBARIUM_DOMAIN else HERBARIUM_DOMAIN\n",
    "\n",
    "        anchor_idx = random.choice(self.label_domain_index[lbl][dom_anchor])\n",
    "        pos_idx = random.choice(self.label_domain_index[lbl][dom_pos])\n",
    "\n",
    "        # Negative: any *other* class from with_labels\n",
    "        neg_lbl = random.choice([c for c in self.with_labels if c != lbl])\n",
    "        \n",
    "        # Negative domain: pick any domain that actually has images for that class\n",
    "        neg_dom_choices = []\n",
    "        for dom in (HERBARIUM_DOMAIN, PHOTO_DOMAIN):\n",
    "            if self.label_domain_index[neg_lbl][dom]:\n",
    "                neg_dom_choices.append(dom)\n",
    "        neg_dom = random.choice(neg_dom_choices)\n",
    "        neg_idx = random.choice(self.label_domain_index[neg_lbl][neg_dom])\n",
    "\n",
    "        return anchor_idx, pos_idx, neg_idx, lbl, neg_lbl\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # idx is ignored; we generate a fresh triplet every time\n",
    "        anchor_idx, pos_idx, neg_idx, lbl, neg_lbl = self._sample_cross_pair()\n",
    "\n",
    "        a = self.base_dataset[anchor_idx]\n",
    "        p = self.base_dataset[pos_idx]\n",
    "        n = self.base_dataset[neg_idx]\n",
    "\n",
    "        return {\n",
    "            \"anchor\": a[\"image\"],\n",
    "            \"positive\": p[\"image\"],\n",
    "            \"negative\": n[\"image\"],\n",
    "            # Metadata (Useful for Metric learning, ignored by Hybrid)\n",
    "            \"anchor_label\": a[\"label\"],\n",
    "            \"positive_label\": p[\"label\"],\n",
    "            \"negative_label\": n[\"label\"],\n",
    "            \"anchor_domain\": a[\"domain\"],\n",
    "            \"positive_domain\": p[\"domain\"],\n",
    "            \"negative_domain\": n[\"domain\"],\n",
    "        }\n",
    "\n",
    "# --- C. Triplet Loader ---\n",
    "\n",
    "triplet_ds = TripletDataset(train_ds, with_set)\n",
    "\n",
    "triplet_loader = DataLoader(\n",
    "    triplet_ds,\n",
    "    batch_size=TRIPLET_BATCH_SIZE, # Defined in Cell 1\n",
    "    shuffle=True,\n",
    "    num_workers=TRIPLET_NUM_WORKERS, # Defined in Cell 1\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "# Warm-up check (from triplet.ipynb)\n",
    "batch = next(iter(triplet_loader))\n",
    "print(\"Triplet batch shapes:\", batch[\"anchor\"].shape, batch[\"positive\"].shape, batch[\"negative\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364c3c81",
   "metadata": {},
   "source": [
    "# Architecture Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "832d893b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models defined: TripletEncoder (Metric) and DinoTriplet (Hybrid)\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 4. Model Architectures\n",
    "# =========================\n",
    "\n",
    "# --- A. Metric Learning Model (Backbone + Projection Head) ---\n",
    "# Source: metric_learning.ipynb\n",
    "# Best for: Pure distance-based learning (Prototypes)\n",
    "\n",
    "class TripletEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        backbone_type: str = \"dinov2_vitb14\",\n",
    "        pretrained: bool = True,\n",
    "        freeze_backbone: bool = False,\n",
    "        proj_hidden_dim: int = 1024,\n",
    "        proj_layers: int = 2,\n",
    "        dropout_p: float = 0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.backbone_type = backbone_type.lower()\n",
    "\n",
    "        # 1. Create Backbone\n",
    "        # Note: In metric_learning.ipynb, there was logic to load a local checkpoint.\n",
    "        # We default to standard timm loading here for safety, but you can uncomment\n",
    "        # the local load logic if you have \"model_best.pth.tar\".\n",
    "        self.backbone = timm.create_model(\n",
    "            \"vit_base_patch14_reg4_dinov2.lvd142m\",\n",
    "            pretrained=pretrained,\n",
    "            num_classes=0  # remove original classifier\n",
    "        )\n",
    "\n",
    "        # 2. Get Feature Dimension\n",
    "        feat_dim = getattr(self.backbone, \"num_features\", None)\n",
    "        if feat_dim is None:\n",
    "            feat_dim = getattr(self.backbone, \"embed_dim\", None)\n",
    "\n",
    "        # 3. Optional Freezing\n",
    "        if freeze_backbone:\n",
    "            for p in self.backbone.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        # 4. Projection Head (feat_dim -> embed_dim)\n",
    "        proj_layers_list = []\n",
    "        in_dim = feat_dim\n",
    "        for i in range(proj_layers - 1):\n",
    "            proj_layers_list.append(nn.Linear(in_dim, proj_hidden_dim))\n",
    "            proj_layers_list.append(nn.BatchNorm1d(proj_hidden_dim))\n",
    "            proj_layers_list.append(nn.ReLU(inplace=True))\n",
    "            if dropout_p > 0:\n",
    "                proj_layers_list.append(nn.Dropout(dropout_p))\n",
    "            in_dim = proj_hidden_dim\n",
    "        proj_layers_list.append(nn.Linear(in_dim, embed_dim))\n",
    "\n",
    "        self.proj_head = nn.Sequential(*proj_layers_list)\n",
    "\n",
    "    def set_backbone_trainable(self, mode: str = \"all\", last_k: int = 2):\n",
    "        \"\"\"\n",
    "        Helper to control fine-tuning depth (Metric Learning approach).\n",
    "        \"\"\"\n",
    "        mode = mode.lower()\n",
    "        # Freeze everything first\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        if mode == \"none\":\n",
    "            return\n",
    "        if mode == \"all\":\n",
    "            for p in self.backbone.parameters():\n",
    "                p.requires_grad = True\n",
    "            return\n",
    "\n",
    "        # Unfreeze last k blocks\n",
    "        if hasattr(self.backbone, \"blocks\"):\n",
    "            blocks = self.backbone.blocks\n",
    "            last_k = min(last_k, len(blocks))\n",
    "            for blk in blocks[-last_k:]:\n",
    "                for p in blk.parameters():\n",
    "                    p.requires_grad = True\n",
    "\n",
    "    def forward_backbone(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.backbone(x)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        feats = self.forward_backbone(x)\n",
    "        z = self.proj_head(feats)\n",
    "        z = F.normalize(z, p=2, dim=-1)\n",
    "        return z\n",
    "\n",
    "\n",
    "# --- B. Hybrid Model (Backbone + Linear Classifier) ---\n",
    "# Source: triplet.ipynb\n",
    "# Best for: Classification + Regularization (CrossEntropy + Triplet Loss)\n",
    "\n",
    "class DinoTriplet(nn.Module):\n",
    "    def __init__(self, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(\n",
    "            \"vit_base_patch14_reg4_dinov2.lvd142m\",\n",
    "            pretrained=True,\n",
    "            num_classes=0,  # embeddings only\n",
    "        )\n",
    "        embed_dim = self.backbone.num_features\n",
    "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x, return_embedding=False, normalize_embedding=True):\n",
    "        feats = self.backbone(x)  # (B, D)\n",
    "        logits = self.classifier(feats)\n",
    "        if return_embedding:\n",
    "            emb = F.normalize(feats, p=2, dim=1) if normalize_embedding else feats\n",
    "            return logits, emb\n",
    "        return logits\n",
    "\n",
    "    def encode(self, x, normalize=True):\n",
    "        \"\"\"Helper for Triplet Loss calculation\"\"\"\n",
    "        feats = self.backbone(x)\n",
    "        if normalize:\n",
    "            return F.normalize(feats, p=2, dim=1)\n",
    "        return feats\n",
    "\n",
    "print(\"Models defined: TripletEncoder (Metric) and DinoTriplet (Hybrid)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e89fb74",
   "metadata": {},
   "source": [
    "# Optimization and Loss Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144b5b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 5. Optimizer, Loss, and Scheduler Setup\n",
    "# =========================\n",
    "\n",
    "# --- Helper 1: Metric Learning Optimizer Groups (Layer-wise Decay) ---\n",
    "# Source: metric_learning.ipynb\n",
    "def get_metric_optimizer_groups(model, lr_head, lr_backbone_max, weight_decay, layer_decay=0.8):\n",
    "    param_groups = []\n",
    "    \n",
    "    # Group A: Projection Head (Highest LR)\n",
    "    if hasattr(model, \"proj_head\"):\n",
    "        head_params = [p for p in model.proj_head.parameters() if p.requires_grad]\n",
    "        if head_params:\n",
    "            param_groups.append({\n",
    "                \"params\": head_params,\n",
    "                \"lr\": lr_head,\n",
    "                \"weight_decay\": weight_decay\n",
    "            })\n",
    "\n",
    "    # Group B: Backbone Blocks (Decaying LR)\n",
    "    # Iterate blocks in reverse: Block 11 -> Block 0\n",
    "    current_lr = lr_backbone_max\n",
    "    \n",
    "    if hasattr(model.backbone, \"blocks\"):\n",
    "        for block in reversed(model.backbone.blocks):\n",
    "            block_params = [p for p in block.parameters() if p.requires_grad]\n",
    "            if block_params:\n",
    "                param_groups.append({\n",
    "                    \"params\": block_params,\n",
    "                    \"lr\": current_lr,\n",
    "                    \"weight_decay\": weight_decay\n",
    "                })\n",
    "                current_lr *= layer_decay # Decay for next block down\n",
    "                \n",
    "    # Add generic backbone params (norm, patch_embed, etc.) if unfrozen\n",
    "    other_backbone_params = []\n",
    "    for name, p in model.backbone.named_parameters():\n",
    "        if \"blocks\" not in name and p.requires_grad:\n",
    "            other_backbone_params.append(p)\n",
    "    if other_backbone_params:\n",
    "         param_groups.append({\n",
    "            \"params\": other_backbone_params,\n",
    "            \"lr\": current_lr, # Lowest LR\n",
    "            \"weight_decay\": weight_decay\n",
    "        })\n",
    "        \n",
    "    return param_groups\n",
    "\n",
    "# --- Helper 2: Hybrid Learning Optimizer Groups ---\n",
    "# Source: triplet.ipynb\n",
    "def get_hybrid_optimizer_groups(model, base_lr=1e-4, decay_rate=0.9):\n",
    "    param_groups = []\n",
    "\n",
    "    # 1. Classifier (Highest LR)\n",
    "    param_groups.append({\n",
    "        \"params\": [p for p in model.classifier.parameters() if p.requires_grad],\n",
    "        \"lr\": base_lr * 1.0,\n",
    "        \"weight_decay\": 1e-4,\n",
    "    })\n",
    "\n",
    "    # 2. Final Norm\n",
    "    param_groups.append({\n",
    "        \"params\": [p for p in model.backbone.norm.parameters() if p.requires_grad],\n",
    "        \"lr\": base_lr * 0.9,\n",
    "        \"weight_decay\": 1e-4,\n",
    "    })\n",
    "\n",
    "    # 3. Backbone Blocks (Scaled LR)\n",
    "    n_blocks = len(model.backbone.blocks)\n",
    "    for i, block in enumerate(model.backbone.blocks):\n",
    "        depth = i\n",
    "        lr_scale = decay_rate ** (n_blocks - depth - 1)\n",
    "        params = [p for p in block.parameters() if p.requires_grad]\n",
    "        if not params: continue\n",
    "        \n",
    "        param_groups.append({\n",
    "            \"params\": params,\n",
    "            \"lr\": base_lr * lr_scale,\n",
    "            \"weight_decay\": 1e-4,\n",
    "        })\n",
    "\n",
    "    # 4. Patch Embed (Lowest LR)\n",
    "    patch_params = [p for p in model.backbone.patch_embed.parameters() if p.requires_grad]\n",
    "    if patch_params:\n",
    "        param_groups.append({\n",
    "            \"params\": patch_params,\n",
    "            \"lr\": base_lr * (decay_rate ** n_blocks),\n",
    "            \"weight_decay\": 1e-4,\n",
    "        })\n",
    "\n",
    "    return param_groups\n",
    "\n",
    "# =========================\n",
    "# CONFIGURATION SELECTOR\n",
    "# =========================\n",
    "\n",
    "# TOGGLE THIS: \"metric\" OR \"hybrid\"\n",
    "TRAINING_MODE = \"hybrid\" \n",
    "print(f\"--> Setting up for mode: {TRAINING_MODE}\")\n",
    "\n",
    "if TRAINING_MODE == \"metric\":\n",
    "    # 1. Instantiate Model\n",
    "    model = TripletEncoder(\n",
    "        embed_dim=EMBED_DIM,\n",
    "        backbone_type=\"dinov2_vitb14\",\n",
    "        pretrained=True,\n",
    "        freeze_backbone=False # Handled by set_backbone_trainable\n",
    "    ).to(device)\n",
    "    \n",
    "    # 2. Fine-tuning Setup\n",
    "    model.set_backbone_trainable(mode=\"last_k\", last_k=5)\n",
    "    \n",
    "    # 3. Optimizer Groups\n",
    "    param_groups = get_metric_optimizer_groups(\n",
    "        model, \n",
    "        lr_head=LR_HEAD, \n",
    "        lr_backbone_max=LR_BACKBONE_MAX, \n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        layer_decay=LAYER_DECAY\n",
    "    )\n",
    "    \n",
    "    # 4. Loss Functions\n",
    "    # Metric learning only uses Triplet Loss\n",
    "    criterion_triplet = nn.TripletMarginLoss(margin=0.2, p=2)\n",
    "    criterion_ce = None # Not used\n",
    "\n",
    "elif TRAINING_MODE == \"hybrid\":\n",
    "    # 1. Instantiate Model\n",
    "    model = DinoTriplet(num_classes=NUM_CLASSES).to(device)\n",
    "    \n",
    "    # 2. Freezing Logic (Manual from triplet.ipynb)\n",
    "    for p in model.backbone.parameters(): p.requires_grad = False\n",
    "    n_blocks = len(model.backbone.blocks)\n",
    "    for i in range(n_blocks - 5, n_blocks): # Unfreeze last 5 blocks\n",
    "        for p in model.backbone.blocks[i].parameters(): p.requires_grad = True\n",
    "    for p in model.backbone.norm.parameters(): p.requires_grad = True\n",
    "    for p in model.classifier.parameters(): p.requires_grad = True\n",
    "    \n",
    "    # 3. Optimizer Groups\n",
    "    param_groups = get_hybrid_optimizer_groups(model, base_lr=1e-4, decay_rate=0.9)\n",
    "    \n",
    "    # 4. Loss Functions\n",
    "    # Hybrid uses both CE (weighted) and Triplet\n",
    "    class_weights = compute_class_weights(train_ds.samples).to(device)\n",
    "    criterion_ce = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    criterion_triplet = nn.TripletMarginLoss(margin=0.3, p=2)\n",
    "\n",
    "# Common Setup\n",
    "optimizer = torch.optim.AdamW(param_groups)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "scaler = amp.GradScaler(enabled=USE_AMP)\n",
    "\n",
    "print(f\"Model: {type(model).__name__}\")\n",
    "print(f\"Trainable Params: {sum(p.numel() for p in model.parameters() if p.requires_grad)/1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab89d4b",
   "metadata": {},
   "source": [
    "# Training and Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa457ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 6. Training and Evaluation Functions\n",
    "# =========================\n",
    "\n",
    "# --- A. Metric Learning Evaluation (Prototypes) ---\n",
    "\n",
    "def build_prototypes(model, loader, device):\n",
    "    model.eval()\n",
    "    proto_sum = torch.zeros(NUM_CLASSES, EMBED_DIM, device=device)\n",
    "    proto_count = torch.zeros(NUM_CLASSES, dtype=torch.long, device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            images = batch[\"image\"].to(device, non_blocking=True)\n",
    "            labels = batch[\"label\"].to(device, non_blocking=True)\n",
    "            domains = batch[\"domain\"].to(device, non_blocking=True)\n",
    "            \n",
    "            # We build prototypes ONLY from the Herbarium domain (source)\n",
    "            mask = (domains == 0) \n",
    "            if mask.sum() == 0: continue\n",
    "\n",
    "            # MetricModel forward returns normalized embeddings\n",
    "            emb = model(images[mask]) \n",
    "            lbls = labels[mask]\n",
    "            \n",
    "            for e, l in zip(emb, lbls):\n",
    "                proto_sum[l] += e\n",
    "                proto_count[l] += 1\n",
    "    \n",
    "    prototypes = torch.zeros_like(proto_sum)\n",
    "    for c in range(NUM_CLASSES):\n",
    "        if proto_count[c] > 0:\n",
    "            prototypes[c] = proto_sum[c] / proto_count[c].float()\n",
    "            prototypes[c] = F.normalize(prototypes[c], p=2, dim=-1)\n",
    "            \n",
    "    return prototypes, proto_count\n",
    "\n",
    "def run_metric_eval(model, loader, prototypes, proto_count, device, with_set, without_set):\n",
    "    model.eval()\n",
    "    k = 5\n",
    "    valid_proto_mask = proto_count > 0\n",
    "\n",
    "    # Initialize counters\n",
    "    stats = {\n",
    "        \"overall\": {\"total\": 0, \"c1\": 0, \"c5\": 0},\n",
    "        \"paired\":  {\"total\": 0, \"c1\": 0, \"c5\": 0}, # \"with_set\"\n",
    "        \"unpaired\": {\"total\": 0, \"c1\": 0, \"c5\": 0} # \"without_set\"\n",
    "    }\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            imgs = batch[\"image\"].to(device, non_blocking=True)\n",
    "            labels = batch[\"label\"].to(device, non_blocking=True)\n",
    "            \n",
    "            emb = model(imgs)\n",
    "            # Similarity = Dot product (since vectors are normalized)\n",
    "            sims = emb @ prototypes.T\n",
    "            # Mask out missing prototypes\n",
    "            sims[:, ~valid_proto_mask] = -1e9\n",
    "            \n",
    "            topk_vals, topk_idx = sims.topk(k=k, dim=1)\n",
    "            preds_top1 = topk_idx[:, 0]\n",
    "\n",
    "            labels_cpu = labels.cpu().tolist()\n",
    "            top1_cpu = preds_top1.cpu().tolist()\n",
    "            topk_cpu = topk_idx.cpu().tolist()\n",
    "\n",
    "            for lbl, p1, pk_list in zip(labels_cpu, top1_cpu, topk_cpu):\n",
    "                # Update Overall\n",
    "                stats[\"overall\"][\"total\"] += 1\n",
    "                if p1 == lbl: stats[\"overall\"][\"c1\"] += 1\n",
    "                if lbl in pk_list: stats[\"overall\"][\"c5\"] += 1\n",
    "\n",
    "                # Update Split Specific\n",
    "                split_key = None\n",
    "                if lbl in with_set: split_key = \"paired\"\n",
    "                elif lbl in without_set: split_key = \"unpaired\"\n",
    "                \n",
    "                if split_key:\n",
    "                    stats[split_key][\"total\"] += 1\n",
    "                    if p1 == lbl: stats[split_key][\"c1\"] += 1\n",
    "                    if lbl in pk_list: stats[split_key][\"c5\"] += 1\n",
    "\n",
    "    # Formatting results\n",
    "    def safe_div(n, d): return n / d if d > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        \"overall_top1\": safe_div(stats[\"overall\"][\"c1\"], stats[\"overall\"][\"total\"]),\n",
    "        \"paired_top1\":  safe_div(stats[\"paired\"][\"c1\"], stats[\"paired\"][\"total\"]),\n",
    "        \"unpaired_top1\": safe_div(stats[\"unpaired\"][\"c1\"], stats[\"unpaired\"][\"total\"]),\n",
    "        \"counts\": {k: v[\"total\"] for k, v in stats.items()}\n",
    "    }\n",
    "\n",
    "\n",
    "# --- B. Hybrid Learning Evaluation (Linear Classifier) ---\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_hybrid_split(model, loader, device, with_set, without_set):\n",
    "    model.eval()\n",
    "    stats = {\n",
    "        \"all\": {\"c1\": 0, \"total\": 0},\n",
    "        \"with\": {\"c1\": 0, \"total\": 0},\n",
    "        \"without\": {\"c1\": 0, \"total\": 0},\n",
    "    }\n",
    "\n",
    "    for batch in loader:\n",
    "        imgs = batch[\"image\"].to(device, non_blocking=True)\n",
    "        labels = batch[\"label\"].to(device, non_blocking=True)\n",
    "\n",
    "        logits = model(imgs) # DinoTriplet returns logits by default\n",
    "        probs = logits.softmax(dim=1)\n",
    "        pred1 = probs.argmax(dim=1)\n",
    "\n",
    "        for i in range(labels.size(0)):\n",
    "            y = labels[i].item()\n",
    "            p1 = (pred1[i].item() == y)\n",
    "\n",
    "            stats[\"all\"][\"total\"] += 1\n",
    "            if p1: stats[\"all\"][\"c1\"] += 1\n",
    "\n",
    "            if y in with_set:\n",
    "                stats[\"with\"][\"total\"] += 1\n",
    "                if p1: stats[\"with\"][\"c1\"] += 1\n",
    "\n",
    "            if y in without_set:\n",
    "                stats[\"without\"][\"total\"] += 1\n",
    "                if p1: stats[\"without\"][\"c1\"] += 1\n",
    "\n",
    "    def get_acc(d): return d[\"c1\"] / d[\"total\"] if d[\"total\"] > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        \"overall_top1\": get_acc(stats[\"all\"]),\n",
    "        \"paired_top1\": get_acc(stats[\"with\"]),\n",
    "        \"unpaired_top1\": get_acc(stats[\"without\"]),\n",
    "    }\n",
    "\n",
    "\n",
    "# --- C. Training Function: Metric Mode ---\n",
    "# Iterates ONLY over Triplet Loader\n",
    "\n",
    "def train_one_epoch_metric(model, loader, optimizer, loss_fn, device, scaler):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    count = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc=\"Train Metric\", leave=False)\n",
    "    for batch in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        anc = batch[\"anchor\"].to(device)\n",
    "        pos = batch[\"positive\"].to(device)\n",
    "        neg = batch[\"negative\"].to(device)\n",
    "        \n",
    "        with autocast(enabled=(device.type == \"cuda\")):\n",
    "            # MetricModel forward returns embeddings\n",
    "            ea = model(anc)\n",
    "            ep = model(pos)\n",
    "            en = model(neg)\n",
    "            loss = loss_fn(ea, ep, en)\n",
    "            \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        count += 1\n",
    "        pbar.set_postfix({\"loss\": f\"{running_loss/count:.4f}\"})\n",
    "    \n",
    "    return running_loss / count\n",
    "\n",
    "\n",
    "# --- D. Training Function: Hybrid Mode ---\n",
    "# Iterates over BOTH Class Loader (CE) and Triplet Loader (Triplet Loss)\n",
    "\n",
    "def train_one_epoch_hybrid(model, cls_loader, trip_loader, optimizer, ce_fn, trip_fn, device, scaler, lambda_t=0.5):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    trip_iter = iter(trip_loader)\n",
    "    \n",
    "    pbar = tqdm(cls_loader, desc=\"Train Hybrid\", leave=False)\n",
    "    for batch in pbar:\n",
    "        imgs = batch[\"image\"].to(device, non_blocking=True)\n",
    "        labels = batch[\"label\"].to(device, non_blocking=True)\n",
    "\n",
    "        # Get Triplet Batch (Cyclic)\n",
    "        try:\n",
    "            trip_batch = next(trip_iter)\n",
    "        except StopIteration:\n",
    "            trip_iter = iter(trip_loader)\n",
    "            trip_batch = next(trip_iter)\n",
    "\n",
    "        anc = trip_batch[\"anchor\"].to(device, non_blocking=True)\n",
    "        pos = trip_batch[\"positive\"].to(device, non_blocking=True)\n",
    "        neg = trip_batch[\"negative\"].to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with autocast(enabled=(device.type == \"cuda\")):\n",
    "            # 1. Classification Loss\n",
    "            logits = model(imgs)\n",
    "            loss_ce = ce_fn(logits, labels)\n",
    "\n",
    "            # 2. Triplet Loss\n",
    "            # DinoTriplet.encode() is used here to get embeddings specifically\n",
    "            anc_emb = model.encode(anc)\n",
    "            pos_emb = model.encode(pos)\n",
    "            neg_emb = model.encode(neg)\n",
    "            loss_trip = trip_fn(anc_emb, pos_emb, neg_emb)\n",
    "\n",
    "            loss = loss_ce + lambda_t * loss_trip\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += loss.item() * imgs.size(0)\n",
    "        \n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    return total_loss / total, correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfae018",
   "metadata": {},
   "source": [
    "# Main Execution Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd4178a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 7. Main Execution Loop\n",
    "# =========================\n",
    "\n",
    "# --- Setup Directories ---\n",
    "CKPT_DIR = PROJECT_ROOT / \"experiments\" / \"merged\" / f\"checkpoints_{TRAINING_MODE}\"\n",
    "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"--> Checkpoints will be saved to: {CKPT_DIR}\")\n",
    "\n",
    "# --- Resume Logic ---\n",
    "# Tries to find the latest epoch or a specific file\n",
    "start_epoch = 0\n",
    "best_primary_metric = 0.0 # Will be \"Unpaired Top-1\" for Metric, \"Overall Top-1\" for Hybrid\n",
    "history = []\n",
    "\n",
    "resume_path = CKPT_DIR / \"last.pt\"\n",
    "if not resume_path.exists():\n",
    "    # Try looking for specific epoch files if \"last.pt\" doesn't exist\n",
    "    chkpts = sorted(list(CKPT_DIR.glob(\"epoch_*.pt\")))\n",
    "    if chkpts:\n",
    "        resume_path = chkpts[-1]\n",
    "\n",
    "if resume_path.exists():\n",
    "    print(f\"--> Resuming from: {resume_path}\")\n",
    "    checkpoint = torch.load(resume_path, map_location=device)\n",
    "    \n",
    "    # Load States\n",
    "    # Note: We use strict=False because switching modes might leave some keys unmatched \n",
    "    # if you try to load a hybrid checkpoint into a metric model (not recommended but handled).\n",
    "    model.load_state_dict(checkpoint[\"model_state\"], strict=False) \n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
    "    if \"scheduler_state\" in checkpoint and checkpoint[\"scheduler_state\"]:\n",
    "        scheduler.load_state_dict(checkpoint[\"scheduler_state\"])\n",
    "    \n",
    "    start_epoch = checkpoint[\"epoch\"] + 1\n",
    "    history = checkpoint.get(\"history\", [])\n",
    "    best_primary_metric = checkpoint.get(\"best_primary_metric\", 0.0)\n",
    "    \n",
    "    print(f\"--> Resuming at Epoch {start_epoch + 1} with Best Metric: {best_primary_metric:.4%}\")\n",
    "else:\n",
    "    print(\"--> Starting from scratch.\")\n",
    "\n",
    "\n",
    "# --- Training Loop ---\n",
    "\n",
    "print(f\"--> Starting Training ({TRAINING_MODE.upper()} Mode)...\")\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # -------------------------------------------\n",
    "    # 1. TRAIN\n",
    "    # -------------------------------------------\n",
    "    if TRAINING_MODE == \"metric\":\n",
    "        # Metric Mode: Train only on triplets\n",
    "        train_loss = train_one_epoch_metric(\n",
    "            model, triplet_loader, optimizer, criterion_triplet, device, scaler\n",
    "        )\n",
    "        # Placeholder for hybrid metrics\n",
    "        train_acc, ce_loss, trip_loss = 0.0, 0.0, train_loss\n",
    "        \n",
    "    else: # Hybrid Mode\n",
    "        # Hybrid Mode: Train on Class + Triplet\n",
    "        train_loss, train_acc = train_one_epoch_hybrid(\n",
    "            model, train_loader, triplet_loader, optimizer, \n",
    "            criterion_ce, criterion_triplet, device, scaler, lambda_t=0.5 # or LAMBDA_TRIPLET\n",
    "        )\n",
    "        # For logging simplicity, we treat the combined loss as \"train_loss\"\n",
    "    \n",
    "    # -------------------------------------------\n",
    "    # 2. EVALUATE\n",
    "    # -------------------------------------------\n",
    "    if TRAINING_MODE == \"metric\":\n",
    "        # Metric Mode: Build prototypes -> Distance check\n",
    "        prototypes, proto_count = build_prototypes(model, train_loader, device)\n",
    "        eval_metrics = run_metric_eval(model, test_loader, prototypes, proto_count, device, with_set, without_set)\n",
    "        \n",
    "        # Primary metric for saving best model: Unpaired Top-1 (Harder task)\n",
    "        current_primary_metric = eval_metrics[\"unpaired_top1\"]\n",
    "        \n",
    "    else: # Hybrid Mode\n",
    "        # Hybrid Mode: Standard Linear Classification\n",
    "        eval_metrics = evaluate_hybrid_split(model, test_loader, device, with_set, without_set)\n",
    "        \n",
    "        # Primary metric for saving best model: Overall Top-1\n",
    "        current_primary_metric = eval_metrics[\"overall_top1\"]\n",
    "\n",
    "    epoch_time = time.time() - start_time\n",
    "    \n",
    "    # -------------------------------------------\n",
    "    # 3. LOGGING\n",
    "    # -------------------------------------------\n",
    "    record = {\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"mode\": TRAINING_MODE,\n",
    "        \"train_loss\": float(train_loss),\n",
    "        \"train_acc\": float(train_acc),\n",
    "        \"eval\": eval_metrics, # Nested dict with all splits\n",
    "        \"time\": epoch_time\n",
    "    }\n",
    "    history.append(record)\n",
    "    \n",
    "    # Console Print\n",
    "    print(\n",
    "        f\"Ep {epoch+1:02d} | \"\n",
    "        f\"Loss: {train_loss:.4f} | \"\n",
    "        f\"Acc: {eval_metrics['overall_top1']:.2%} (All) | \"\n",
    "        f\"{eval_metrics['paired_top1']:.2%} (Paired) | \"\n",
    "        f\"{eval_metrics['unpaired_top1']:.2%} (Unpaired) | \"\n",
    "        f\"Time: {epoch_time:.0f}s\"\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------\n",
    "    # 4. SAVING\n",
    "    # -------------------------------------------\n",
    "    save_dict = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optimizer_state\": optimizer.state_dict(),\n",
    "        \"scheduler_state\": scheduler.state_dict(),\n",
    "        \"history\": history,\n",
    "        \"best_primary_metric\": best_primary_metric,\n",
    "        \"mode\": TRAINING_MODE\n",
    "    }\n",
    "    \n",
    "    # Save Latest\n",
    "    torch.save(save_dict, CKPT_DIR / \"last.pt\")\n",
    "    \n",
    "    # Save Best\n",
    "    if current_primary_metric > best_primary_metric:\n",
    "        print(f\"--> New Best Model! ({best_primary_metric:.2%} -> {current_primary_metric:.2%})\")\n",
    "        best_primary_metric = current_primary_metric\n",
    "        torch.save(save_dict, CKPT_DIR / \"best_model.pt\")\n",
    "        \n",
    "    # Periodic Save (Optional)\n",
    "    # if (epoch + 1) % 5 == 0:\n",
    "    #     torch.save(save_dict, CKPT_DIR / f\"epoch_{epoch+1}.pt\")\n",
    "    \n",
    "    # Dump History JSON\n",
    "    with open(CKPT_DIR / \"history.json\", \"w\") as f:\n",
    "        json.dump(history, f, indent=4)\n",
    "\n",
    "    # Step Scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "print(\"Training Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9f7671",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595e9257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 8. Visualization of Results\n",
    "# =========================\n",
    "\n",
    "# If history is empty (e.g. just started), try loading it\n",
    "if not history:\n",
    "    history_path = CKPT_DIR / \"history.json\"\n",
    "    if history_path.exists():\n",
    "        with open(history_path, \"r\") as f:\n",
    "            history = json.load(f)\n",
    "\n",
    "if history:\n",
    "    epochs = [h[\"epoch\"] for h in history]\n",
    "    train_loss = [h[\"train_loss\"] for h in history]\n",
    "    \n",
    "    # Extract accuracies (structure depends on mode)\n",
    "    # Both modes save 'eval' dict with 'overall_top1', 'paired_top1', 'unpaired_top1'\n",
    "    overall_acc = [h[\"eval\"][\"overall_top1\"] for h in history]\n",
    "    paired_acc = [h[\"eval\"][\"paired_top1\"] for h in history]\n",
    "    unpaired_acc = [h[\"eval\"][\"unpaired_top1\"] for h in history]\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Plot 1: Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_loss, marker='o', label='Train Loss')\n",
    "    plt.title(f\"Training Loss ({TRAINING_MODE})\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot 2: Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, overall_acc, marker='o', label='Overall Top-1')\n",
    "    plt.plot(epochs, paired_acc, marker='s', linestyle='--', label='Paired Top-1')\n",
    "    plt.plot(epochs, unpaired_acc, marker='^', linestyle='--', label='Unpaired Top-1')\n",
    "    plt.title(f\"Evaluation Accuracy ({TRAINING_MODE})\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Final Unpaired Accuracy: {unpaired_acc[-1]:.2%}\")\n",
    "else:\n",
    "    print(\"No history found to plot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0438bae",
   "metadata": {},
   "source": [
    "# Evaluation model's performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5da3a23",
   "metadata": {},
   "source": [
    "## Save and Load Protoype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e75a05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Saving Prototypes ---\n",
    "def save_prototypes(prototypes, counts, path):\n",
    "    \"\"\"Saves prototypes and their counts to a file.\"\"\"\n",
    "    print(f\"Saving prototypes to {path}...\")\n",
    "    torch.save({\n",
    "        \"prototypes\": prototypes.cpu(), # Move to CPU for storage\n",
    "        \"counts\": counts.cpu()\n",
    "    }, path)\n",
    "    print(\"Done!\")\n",
    "\n",
    "# --- Loading Prototypes ---\n",
    "def load_prototypes(path):\n",
    "    \"\"\"Loads prototypes from file, ready for inference.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"No prototype file found at {path}\")\n",
    "    \n",
    "    print(f\"Loading prototypes from {path}...\")\n",
    "    data = torch.load(path, map_location=device)\n",
    "    return data[\"prototypes\"]\n",
    "\n",
    "def load_dual_prototypes(path, device=None):\n",
    "    \"\"\"\n",
    "    Loads prototypes from a .pt file.\n",
    "    Handles both raw Tensor saves and Dictionary saves.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Prototype file not found at: {path}\")\n",
    "    \n",
    "    # Determine device automatically if not provided\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "    print(f\"Loading prototypes from {path}...\")\n",
    "    \n",
    "    # Load the file mapping to the correct device immediately\n",
    "    loaded_data = torch.load(path, map_location=device, weights_only=False)\n",
    "\n",
    "    # CASE 1: The file is just the Tensor (Result of the create_prototypes function we fixed earlier)\n",
    "    if isinstance(loaded_data, torch.Tensor):\n",
    "        return loaded_data.to(device)\n",
    "\n",
    "    # CASE 2: The file is a Dictionary containing the tensor\n",
    "    elif isinstance(loaded_data, dict):\n",
    "        # Check common keys\n",
    "        if \"prototypes\" in loaded_data:\n",
    "            protos = loaded_data[\"prototypes\"]\n",
    "            print(f\"Loaded dictionary prototypes of shape: {protos.shape}\")\n",
    "            return protos.to(device)\n",
    "        elif \"protos\" in loaded_data:\n",
    "            protos = loaded_data[\"protos\"]\n",
    "            return protos.to(device)\n",
    "        else:\n",
    "            # Fallback: check if values are tensors\n",
    "            keys = list(loaded_data.keys())\n",
    "            raise KeyError(f\"Could not find 'prototypes' key in dictionary. Available keys: {keys}\")\n",
    "\n",
    "    else:\n",
    "        raise TypeError(f\"Unsupported file format. Expected Tensor or Dict, got {type(loaded_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e9dd4b",
   "metadata": {},
   "source": [
    "## Create Prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0b73822",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_and_save_prototypes(model, train_ds, save_path, batch_size=32):\n",
    "    \"\"\"\n",
    "    Calculates prototypes from the training set (Herbarium domain only) \n",
    "    and saves them to the specified path.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # 1. Build Prototypes from Train (Herbarium samples only)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    \n",
    "    proto_sum = None\n",
    "    # Note: Ensure NUM_CLASSES and device are defined in your scope or passed as args\n",
    "    proto_count = torch.zeros(NUM_CLASSES, dtype=torch.long, device=device)\n",
    "    \n",
    "    print(f\"Building Prototypes from {len(train_ds)} samples...\")\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(train_loader, desc=\"Prototypes\"):\n",
    "            imgs = batch[\"image\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "            domains = batch[\"domain\"].to(device)\n",
    "            \n",
    "            # Domain 0 is Herbarium\n",
    "            mask = (domains == 0)\n",
    "            if mask.sum() == 0: continue\n",
    "            \n",
    "            emb = model(imgs[mask])\n",
    "            lbls = labels[mask]\n",
    "            \n",
    "            if proto_sum is None:\n",
    "                proto_sum = torch.zeros(NUM_CLASSES, emb.shape[1], device=device)\n",
    "                \n",
    "            for e, l in zip(emb, lbls):\n",
    "                proto_sum[l] += e\n",
    "                proto_count[l] += 1\n",
    "\n",
    "    # Check if we actually found herbarium samples\n",
    "    if proto_sum is None:\n",
    "        raise RuntimeError(\"No Herbarium images found in train_ds! Check dataset loading.\")\n",
    "\n",
    "    prototypes = torch.zeros_like(proto_sum)\n",
    "    for c in range(NUM_CLASSES):\n",
    "        if proto_count[c] > 0:\n",
    "            prototypes[c] = proto_sum[c] / proto_count[c]\n",
    "            prototypes[c] = F.normalize(prototypes[c], p=2, dim=-1)\n",
    "\n",
    "    # 2. Save Prototypes\n",
    "    if not save_path.parent.exists():\n",
    "        save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "    # Removed the \"if exists\" check so this function forces a save/update\n",
    "    save_prototypes(prototypes, proto_count, save_path)\n",
    "    \n",
    "    print(f\"Prototypes saved to {save_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aaaca79",
   "metadata": {},
   "source": [
    "## Evaluation Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24054d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_with_prototypes(model, test_ds, prototypes, with_set, without_set, batch_size=32):\n",
    "    \"\"\"\n",
    "    Evaluates model using pre-loaded prototypes.\n",
    "    - Fixes Windows DataLoader crash (num_workers=0).\n",
    "    - Fixes Architecture mismatch (checks for .encode() method).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    prototypes = prototypes.to(device)\n",
    "    \n",
    "    # FIX 1: Set num_workers=0 for Windows compatibility\n",
    "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    \n",
    "    stats = {k: {\"correct\": 0, \"total\": 0} for k in [\"all\", \"with\", \"without\"]}\n",
    "    \n",
    "    print(\"Testing with loaded prototypes...\")\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "            imgs = batch[\"image\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "            \n",
    "            # FIX 2: Handle Architecture Difference\n",
    "            # If the model has an 'encode' method (DinoTriplet), use it to get embeddings.\n",
    "            # If not (TripletEncoder), use standard forward.\n",
    "            if hasattr(model, 'encode'):\n",
    "                emb = model.encode(imgs)\n",
    "            else:\n",
    "                emb = model(imgs)\n",
    "            \n",
    "            # Cosine Similarity\n",
    "            sims = emb @ prototypes.T\n",
    "            preds = sims.argmax(dim=1)\n",
    "            \n",
    "            for p, l in zip(preds, labels):\n",
    "                lbl_item = l.item()\n",
    "                is_correct = (p == l).item()\n",
    "                \n",
    "                stats[\"all\"][\"total\"] += 1\n",
    "                if is_correct: stats[\"all\"][\"correct\"] += 1\n",
    "                \n",
    "                if lbl_item in with_set:\n",
    "                    stats[\"with\"][\"total\"] += 1\n",
    "                    if is_correct: stats[\"with\"][\"correct\"] += 1\n",
    "                elif lbl_item in without_set:\n",
    "                    stats[\"without\"][\"total\"] += 1\n",
    "                    if is_correct: stats[\"without\"][\"correct\"] += 1\n",
    "    \n",
    "    return {\n",
    "        \"Overall\": stats[\"all\"][\"correct\"] / stats[\"all\"][\"total\"] if stats[\"all\"][\"total\"] > 0 else 0,\n",
    "        \"With-Pair\": stats[\"with\"][\"correct\"] / stats[\"with\"][\"total\"] if stats[\"with\"][\"total\"] > 0 else 0,\n",
    "        \"Without-Pair\": stats[\"without\"][\"correct\"] / stats[\"without\"][\"total\"] if stats[\"without\"][\"total\"] > 0 else 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de087dce",
   "metadata": {},
   "source": [
    "## Running Models' Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc64bc7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading prototypes from c:\\Users\\William\\School\\Swinburne\\Computer Science\\2025 Semester 2\\COS30082 Applied Machine Learning\\Assignment 2\\Approach3\\experiments\\ensemble\\prototype_metric.pt...\n",
      "Testing with loaded prototypes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|| 7/7 [00:13<00:00,  1.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric Learning Results: {'Overall': 0.7101449275362319, 'With-Pair': 0.7777777777777778, 'Without-Pair': 0.5185185185185185}\n",
      "Loading prototypes from c:\\Users\\William\\School\\Swinburne\\Computer Science\\2025 Semester 2\\COS30082 Applied Machine Learning\\Assignment 2\\Approach3\\experiments\\ensemble\\prototypes_triplet.pt...\n",
      "Testing with loaded prototypes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|| 7/7 [00:10<00:00,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triplet Results: {'Overall': 0.7681159420289855, 'With-Pair': 0.9607843137254902, 'Without-Pair': 0.2222222222222222}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define paths (Adjust if your folder structure is different)\n",
    "# metric checkpoint is usually in experiments/2_stream/checkpoints_k5\n",
    "path_m = PROJECT_ROOT / \"experiments\" / \"ensemble\" / \"metric.pt\"\n",
    "save_path_m = PROJECT_ROOT / \"experiments\" / \"ensemble\" / \"prototype_metric.pt\"\n",
    "# Triplet checkpoint is usually in experiments/triplet/checkpoints (or similar)\n",
    "path_tr = PROJECT_ROOT / \"experiments\" / \"ensemble\" / \"triplet.pt\"\n",
    "save_path_tr = PROJECT_ROOT / \"experiments\" / \"ensemble\" / \"prototypes_triplet.pt\"\n",
    "\n",
    "# ===================================\n",
    "# --- 1. Evaluate Metric Learning ---\n",
    "# ===================================\n",
    "\n",
    "model_metric = TripletEncoder(embed_dim=512).to(device)\n",
    "state = torch.load(path_m, map_location=device)\n",
    "# Handle wrapped state dicts\n",
    "if 'model_state' in state: state = state['model_state']\n",
    "model_metric.load_state_dict(state, strict=True)\n",
    "\n",
    "if not save_path_m.exists():\n",
    "    # Create and Save Prototypes\n",
    "    create_and_save_prototypes(model_metric, train_ds, save_path_m)\n",
    "\n",
    "# Load Prototypes\n",
    "prototypes_2s = load_prototypes(save_path_m)\n",
    "res_2s = evaluate_model_with_prototypes(model_metric, test_ds, prototypes_2s, with_set, without_set)\n",
    "print(f\"Metric Learning Results: {res_2s}\")\n",
    "\n",
    "# ===================================\n",
    "# -------- 2. Evaluate Triplet ------\n",
    "# ===================================\n",
    "\n",
    "model_triplet = DinoTriplet(num_classes=NUM_CLASSES).to(device)\n",
    "state = torch.load(path_tr, map_location=device)\n",
    "if 'model_state' in state: state = state['model_state']\n",
    "model_triplet.load_state_dict(state, strict=True)\n",
    "\n",
    "if not save_path_tr.exists():\n",
    "    # Create and Save Prototypes\n",
    "    create_and_save_prototypes(model_triplet, train_ds, save_path_tr)\n",
    "\n",
    "# Load Prototypes\n",
    "prototypes_tr = load_prototypes(save_path_tr)\n",
    "# Evaluate\n",
    "res_tr = evaluate_model_with_prototypes(model_triplet, test_ds, prototypes_tr, with_set, without_set)\n",
    "print(f\"Triplet Results: {res_tr}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69541e3",
   "metadata": {},
   "source": [
    "# Ensemble Predicter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a3fbe69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(threshold, return_top5=False):\n",
    "    \"\"\"\n",
    "    Hybrid Strategy:\n",
    "    1. Get prediction from 'model_triplet' (The Specialist).\n",
    "    2. Verify it using 'model_metric' (The Generalist) by checking the distance \n",
    "       in the 2-stream embedding space.\n",
    "    3. If distance < threshold: TRUST model_triplet.\n",
    "    4. If distance > threshold: REJECT model_triplet, use model_metric's nearest neighbor.\n",
    "    \n",
    "    If return_top5=True, also returns top-5 predictions for each sample.\n",
    "    \"\"\"\n",
    "    hybrid_preds = []\n",
    "    hybrid_top5 = []\n",
    "    targets = []\n",
    "\n",
    "    # Define the Specialist(well in paired classes) and Generalist(well in unpaired classes) prototypes\n",
    "    Specialist = model_triplet.eval()\n",
    "    Generalist = model_metric.eval()\n",
    "    \n",
    "    # FIX 1: Set num_workers=0 to prevent Windows crash\n",
    "    test_loader = DataLoader(test_ds, batch_size=32, shuffle=False, num_workers=0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Hybrid Inference\"): \n",
    "            imgs = batch[\"image\"].to(device)\n",
    "            lbls = batch[\"label\"].to(device)\n",
    "            targets.extend(lbls.cpu().numpy())\n",
    "            \n",
    "            # --- Step 1: Get Candidate from Specialist (model_triplet) ---\n",
    "            # FIX 2: Use .encode() to get Embeddings (768d), NOT Logits (100d)\n",
    "            emb_tr = Specialist.encode(imgs) \n",
    "            \n",
    "            # Now shapes will match: (32x768) @ (768x100) -> (32x100)\n",
    "            sims_tr = emb_tr @ prototypes_tr.T\n",
    "            pred_class_tr = sims_tr.argmax(dim=1) # The class model_triplet thinks it is\n",
    "            top5_tr = sims_tr.topk(5, dim=1)[1]\n",
    "\n",
    "            # --- Step 2: Get Embedding from Generalist (model_metric) ---\n",
    "            # Generalist (TripletEncoder) always returns embeddings (512d)\n",
    "            emb_2s = Generalist(imgs)\n",
    "\n",
    "            # --- Step 3: Distance Verification ---\n",
    "            # Look up the 2-Stream prototype for the class model_triplet predicted.\n",
    "            proposed_protos = prototypes_2s[pred_class_tr] # [Batch, 512]\n",
    "            \n",
    "            # Calculate Euclidean Distance\n",
    "            dists = torch.norm(emb_2s - proposed_protos, dim=1) # [Batch]\n",
    "            \n",
    "            # --- Step 4: The Decision ---\n",
    "            batch_preds = []\n",
    "            batch_top5 = []\n",
    "            for i in range(len(imgs)):\n",
    "\n",
    "                if dists[i].item() < threshold:\n",
    "                    # TRUST SPECIALIST\n",
    "                    batch_preds.append(pred_class_tr[i].item())\n",
    "                    if return_top5:\n",
    "                        batch_top5.append(top5_tr[i].cpu().tolist())\n",
    "                else:\n",
    "                    # REJECT SPECIALIST -> FALLBACK TO GENERALIST\n",
    "                    sims_2s = emb_2s[i].unsqueeze(0) @ prototypes_2s.T\n",
    "                    nn_class = sims_2s.argmax().item()\n",
    "                    batch_preds.append(nn_class)\n",
    "                    if return_top5:\n",
    "                        top5_2s = sims_2s.topk(5, dim=1)[1][0].cpu().tolist()\n",
    "                        batch_top5.append(top5_2s)\n",
    "            \n",
    "            hybrid_preds.extend(batch_preds)\n",
    "            if return_top5:\n",
    "                hybrid_top5.extend(batch_top5)\n",
    "\n",
    "    if return_top5:\n",
    "        return hybrid_preds, targets, hybrid_top5\n",
    "    return hybrid_preds, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b0e8c6",
   "metadata": {},
   "source": [
    "## Tuning Hybrid Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "396b028c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Tuning Hybrid Threshold ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hybrid Inference: 100%|| 7/7 [00:19<00:00,  2.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold 0.90 -> Overall Acc: 0.7923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hybrid Inference: 100%|| 7/7 [00:19<00:00,  2.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold 0.91 -> Overall Acc: 0.7971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hybrid Inference: 100%|| 7/7 [00:19<00:00,  2.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold 0.92 -> Overall Acc: 0.7971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hybrid Inference: 100%|| 7/7 [00:19<00:00,  2.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold 0.93 -> Overall Acc: 0.8068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hybrid Inference: 100%|| 7/7 [00:19<00:00,  2.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold 0.94 -> Overall Acc: 0.8068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hybrid Inference: 100%|| 7/7 [00:19<00:00,  2.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold 0.95 -> Overall Acc: 0.8068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hybrid Inference: 100%|| 7/7 [00:19<00:00,  2.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold 0.96 -> Overall Acc: 0.8068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hybrid Inference: 100%|| 7/7 [00:19<00:00,  2.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold 0.97 -> Overall Acc: 0.8068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hybrid Inference: 100%|| 7/7 [00:19<00:00,  2.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold 0.98 -> Overall Acc: 0.8068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hybrid Inference: 100%|| 7/7 [00:19<00:00,  2.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold 0.99 -> Overall Acc: 0.8068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hybrid Inference: 100%|| 7/7 [00:19<00:00,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold 1.00 -> Overall Acc: 0.7923\n",
      "Best threshold: 0.93 Acc: 0.8067632850241546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Tuning Hybrid Threshold ---\n",
    "print(\"\\n--- Tuning Hybrid Threshold ---\")\n",
    "\n",
    "# Range of distances to test. \n",
    "# Lower (0.3) = Strict, trusts Generalist more.\n",
    "# Higher (1.2) = Loose, trusts Specialist more.\n",
    "# thresholds = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2]\n",
    "thresholds = [0.90, 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99, 1.00]\n",
    "\n",
    "\n",
    "best_acc = 0.0\n",
    "best_threshold = 0.0\n",
    "\n",
    "for th in thresholds:\n",
    "    preds, tgts = run_inference(th)\n",
    "    correct = sum(1 for p, t in zip(preds, tgts) if p == t)\n",
    "    acc = correct / len(tgts)\n",
    "    print(f\"Threshold {th:.2f} -> Overall Acc: {acc:.4f}\")\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_threshold = th\n",
    "\n",
    "print(\"Best threshold:\", best_threshold, \"Acc:\", best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e464c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Final Breakdown with Threshold 0.93...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hybrid Inference: 100%|| 7/7 [00:19<00:00,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid Results:\n",
      "  Overall Top-1: 80.68%\n",
      "  Overall Top-5: 89.37%\n",
      "  With-Pair Top-1: 94.77%\n",
      "  With-Pair Top-5: 97.39%\n",
      "  Without-Pair Top-1: 40.74%\n",
      "  Without-Pair Top-5: 66.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Final Detailed Evaluation ---\n",
    "# best_threshold = 0.93\n",
    "print(f\"\\nRunning Final Breakdown with Threshold {best_threshold}...\")\n",
    "final_preds, final_targets, final_top5 = run_inference(best_threshold, return_top5=True)\n",
    "\n",
    "# Manual Breakdown Calculation (Top-1 and Top-5)\n",
    "stats = {\n",
    "    k: {\"correct1\": 0, \"correct5\": 0, \"total\": 0} \n",
    "    for k in [\"all\", \"with\", \"without\"]\n",
    "}\n",
    "\n",
    "for p, t, top5 in zip(final_preds, final_targets, final_top5):\n",
    "    is_correct1 = (p == t)\n",
    "    is_correct5 = (t in top5)\n",
    "    \n",
    "    stats[\"all\"][\"total\"] += 1\n",
    "    if is_correct1: stats[\"all\"][\"correct1\"] += 1\n",
    "    if is_correct5: stats[\"all\"][\"correct5\"] += 1\n",
    "    \n",
    "    if t in with_set:\n",
    "        stats[\"with\"][\"total\"] += 1\n",
    "        if is_correct1: stats[\"with\"][\"correct1\"] += 1\n",
    "        if is_correct5: stats[\"with\"][\"correct5\"] += 1\n",
    "    elif t in without_set:\n",
    "        stats[\"without\"][\"total\"] += 1\n",
    "        if is_correct1: stats[\"without\"][\"correct1\"] += 1\n",
    "        if is_correct5: stats[\"without\"][\"correct5\"] += 1\n",
    "\n",
    "results = {\n",
    "    \"Overall Top-1\": stats[\"all\"][\"correct1\"] / stats[\"all\"][\"total\"],\n",
    "    \"Overall Top-5\": stats[\"all\"][\"correct5\"] / stats[\"all\"][\"total\"],\n",
    "    \"With-Pair Top-1\": stats[\"with\"][\"correct1\"] / stats[\"with\"][\"total\"],\n",
    "    \"With-Pair Top-5\": stats[\"with\"][\"correct5\"] / stats[\"with\"][\"total\"],\n",
    "    \"Without-Pair Top-1\": stats[\"without\"][\"correct1\"] / stats[\"without\"][\"total\"],\n",
    "    \"Without-Pair Top-5\": stats[\"without\"][\"correct5\"] / stats[\"without\"][\"total\"]\n",
    "}\n",
    "\n",
    "print(\"Hybrid Results:\")\n",
    "for metric, value in results.items():\n",
    "    print(f\"  {metric}: {value:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
