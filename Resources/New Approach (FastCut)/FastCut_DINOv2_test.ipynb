{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14279,
     "status": "ok",
     "timestamp": 1764140014358,
     "user": {
      "displayName": "Esther Chai",
      "userId": "05451503381207659663"
     },
     "user_tz": -480
    },
    "id": "J9fCLB-yFUsd",
    "outputId": "c5941ffa-4094-41e0-fea5-5d03741a6ec5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Install packages\n",
    "!pip install --upgrade pip\n",
    "!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install tqdm matplotlib pillow\n",
    "\n",
    "# Clone the repo using Python logic instead of Bash\n",
    "repo_url = \"https://github.com/taesungp/contrastive-unpaired-translation.git\"\n",
    "repo_name = \"contrastive-unpaired-translation\"\n",
    "\n",
    "if not os.path.exists(repo_name):\n",
    "    print(f\"Cloning {repo_name}...\")\n",
    "    !git clone {repo_url}\n",
    "else:\n",
    "    print(f\"{repo_name} already exists. Skipping clone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40159,
     "status": "ok",
     "timestamp": 1764140054541,
     "user": {
      "displayName": "Esther Chai",
      "userId": "05451503381207659663"
     },
     "user_tz": -480
    },
    "id": "KNcjMkc7FpN5",
    "outputId": "9251d2df-6020-48d5-b9d7-fee4b433da24"
   },
   "outputs": [],
   "source": [
    "# Block 1 - Settings & imports\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# ---------- EDIT if different ----------\n",
    "PROJECT_ROOT = Path.cwd().parents[1] / \"Assignment 2\" / \"Esther\"\n",
    "DATA_ROOT  = PROJECT_ROOT / \"AML_project_herbarium_dataset\"\n",
    "TRAIN_LIST = DATA_ROOT / \"list/train.txt\"\n",
    "TEST_LIST  = DATA_ROOT / \"list/test.txt\"\n",
    "REMAPPED_DIR = DATA_ROOT / \"list/remapped\"\n",
    "TRAIN_REMAP = REMAPPED_DIR / \"train_remapped.txt\"\n",
    "TRAIN_WITH_SYN = REMAPPED_DIR / \"train_remapped_with_synth.txt\"\n",
    "SPECIES_LIST_PATH = DATA_ROOT / \"list/species_list.txt\"  # uploaded file path\n",
    "CLASS_WITHOUT_PAIRS = DATA_ROOT / \"list/class_without_pairs.txt\"  # expects this file\n",
    "FASTCUT_WORKDIR = PROJECT_ROOT / \"models/fastcut_work\"\n",
    "FASTCUT_ROOT = FASTCUT_WORKDIR / \"fastcut_data\"\n",
    "FASTCUT_RESULTS = PROJECT_ROOT / \"models/fastcut_results/herb2field_cut_fast/test_latest/images\"\n",
    "FAKE_B_DIR = FASTCUT_RESULTS / \"fake_B\"\n",
    "SYN_DEST_DIR = Path(DATA_ROOT) / \"synthetic\"\n",
    "\n",
    "# GPU ID (used for shell commands)\n",
    "GPU_ID = 0\n",
    "\n",
    "# training / finetune params (you can tune later)\n",
    "EPOCHS_HEAD = 20\n",
    "EPOCHS_FINE = 20\n",
    "LR_HEAD = 1e-4\n",
    "LR_FINE = 1e-5\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "# convenience prints\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"DATA_ROOT:\", DATA_ROOT)\n",
    "print(\"TRAIN_REMAP exists:\", TRAIN_REMAP.exists())\n",
    "print(\"SPECIES_LIST path (uploaded):\", SPECIES_LIST_PATH.exists(), SPECIES_LIST_PATH)\n",
    "print(\"FASTCUT fake_B exists:\", FAKE_B_DIR.exists())\n",
    "print(\"FAKE_B count (if exists):\", len(list(FAKE_B_DIR.glob(\"*.png\"))) if FAKE_B_DIR.exists() else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 663,
     "status": "ok",
     "timestamp": 1764140055220,
     "user": {
      "displayName": "Esther Chai",
      "userId": "05451503381207659663"
     },
     "user_tz": -480
    },
    "id": "SMITl7oRGAgn",
    "outputId": "600d544f-af31-4de2-b135-e68d805d5a3c"
   },
   "outputs": [],
   "source": [
    "# Block 2 - build mappings from species_list.txt\n",
    "orig_to_new = {}\n",
    "new_to_orig = {}\n",
    "\n",
    "if not SPECIES_LIST_PATH.exists():\n",
    "    raise FileNotFoundError(f\"species_list.txt not found at {SPECIES_LIST_PATH}. Please upload it.\")\n",
    "\n",
    "with open(SPECIES_LIST_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for new_idx, line in enumerate(f):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        parts = line.split(\";\")\n",
    "        orig_str = parts[0].strip()\n",
    "        try:\n",
    "            orig_id = int(orig_str)\n",
    "        except:\n",
    "            # try to coerce\n",
    "            orig_id = int(orig_str.split()[0])\n",
    "        orig_to_new[orig_id] = new_idx\n",
    "        new_to_orig[new_idx] = orig_id\n",
    "\n",
    "print(\"Total species mapped:\", len(orig_to_new))\n",
    "# print a few examples\n",
    "for k in list(orig_to_new.keys())[:10]:\n",
    "    print(k, \"->\", orig_to_new[k])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 581,
     "status": "ok",
     "timestamp": 1764140055820,
     "user": {
      "displayName": "Esther Chai",
      "userId": "05451503381207659663"
     },
     "user_tz": -480
    },
    "id": "bc_2Xff5HU8G",
    "outputId": "34af8002-88dc-4813-d4ca-cadf3dfdf349"
   },
   "outputs": [],
   "source": [
    "# Block 3 - class_without_pairs -> unpaired_new_ids\n",
    "if not CLASS_WITHOUT_PAIRS.exists():\n",
    "    print(\"Warning: class_without_pairs.txt not found at\", CLASS_WITHOUT_PAIRS)\n",
    "    # If you don't have the file, you can paste the original IDs manually as a list here:\n",
    "    # unpaired_orig_ids = [105951, 106387, ...]\n",
    "    raise FileNotFoundError(\"class_without_pairs.txt missing; place it in dataset/list/remapped/\")\n",
    "\n",
    "unpaired_orig_ids = []\n",
    "with open(CLASS_WITHOUT_PAIRS, \"r\", encoding=\"utf-8\") as f:\n",
    "    for ln in f:\n",
    "        ln = ln.strip()\n",
    "        if not ln: continue\n",
    "        try:\n",
    "            unpaired_orig_ids.append(int(ln.split()[0]))\n",
    "        except:\n",
    "            try:\n",
    "                unpaired_orig_ids.append(int(ln))\n",
    "            except:\n",
    "                print(\"Could not parse line:\", ln)\n",
    "\n",
    "# map to new indices\n",
    "unpaired_new_ids = set()\n",
    "missing_orig = []\n",
    "for oid in unpaired_orig_ids:\n",
    "    if oid in orig_to_new:\n",
    "        unpaired_new_ids.add(orig_to_new[oid])\n",
    "    else:\n",
    "        missing_orig.append(oid)\n",
    "\n",
    "print(\"Unpaired original count:\", len(unpaired_orig_ids))\n",
    "print(\"Mapped to new indices count:\", len(unpaired_new_ids))\n",
    "if missing_orig:\n",
    "    print(\"Missing orig IDs from species_list mapping (should be none):\", missing_orig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 3.5 - Generate train_remapped.txt\n",
    "# This reads TRAIN_LIST, applies the mapping from Block 2 (orig_to_new),\n",
    "# and saves the result to TRAIN_REMAP.\n",
    "\n",
    "print(f\"Generating {TRAIN_REMAP} from {TRAIN_LIST}...\")\n",
    "\n",
    "# 1. Ensure the directory exists\n",
    "TRAIN_REMAP.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "remapped_lines = []\n",
    "skipped_count = 0\n",
    "\n",
    "# 2. Open original train list and map IDs\n",
    "if not TRAIN_LIST.exists():\n",
    "    raise FileNotFoundError(f\"Original train list not found at {TRAIN_LIST}\")\n",
    "\n",
    "with open(TRAIN_LIST, \"r\", encoding=\"utf-8\") as f_in:\n",
    "    for line in f_in:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        \n",
    "        parts = line.split()\n",
    "        if len(parts) < 2:\n",
    "            continue\n",
    "            \n",
    "        rel_path = parts[0]\n",
    "        try:\n",
    "            # Assumes the original ID is the second element\n",
    "            orig_id = int(parts[1])\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "        # 3. Apply Mapping (orig_id -> new_id)\n",
    "        if orig_id in orig_to_new:\n",
    "            new_id = orig_to_new[orig_id]\n",
    "            # Write: relative/path/to/image.jpg new_id\n",
    "            remapped_lines.append(f\"{rel_path} {new_id}\")\n",
    "        else:\n",
    "            skipped_count += 1\n",
    "\n",
    "# 4. Save the new file\n",
    "with open(TRAIN_REMAP, \"w\", encoding=\"utf-8\") as f_out:\n",
    "    for line in remapped_lines:\n",
    "        f_out.write(line + \"\\n\")\n",
    "\n",
    "print(f\"Done! Created {TRAIN_REMAP}\")\n",
    "print(f\"Total lines written: {len(remapped_lines)}\")\n",
    "print(f\"Skipped (ID not in species_list): {skipped_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 742,
     "status": "ok",
     "timestamp": 1764140056575,
     "user": {
      "displayName": "Esther Chai",
      "userId": "05451503381207659663"
     },
     "user_tz": -480
    },
    "id": "n3ZVTU50IAJE",
    "outputId": "2b3cfe90-6de9-4135-e260-8d2236292337"
   },
   "outputs": [],
   "source": [
    "# Block 4 - check train_remapped.txt\n",
    "if not TRAIN_REMAP.exists():\n",
    "    raise FileNotFoundError(\"train_remapped.txt not found at expected location: \" + str(TRAIN_REMAP))\n",
    "\n",
    "lines = []\n",
    "with open(TRAIN_REMAP, \"r\", encoding=\"utf-8\") as f:\n",
    "    for ln in f:\n",
    "        ln = ln.strip()\n",
    "        if not ln: continue\n",
    "        parts = ln.split()\n",
    "        if len(parts) < 2:\n",
    "            continue\n",
    "        lines.append((parts[0], int(parts[1])))\n",
    "\n",
    "print(\"First 10 lines of train_remapped.txt:\")\n",
    "for item in lines[:10]:\n",
    "    print(item)\n",
    "print(\"Total remapped train lines:\", len(lines))\n",
    "\n",
    "# Count how many herbarium images belong to unpaired_new_ids\n",
    "count_herb_unpaired = sum(1 for rel, cls in lines if cls in unpaired_new_ids and \"herbarium\" in rel.lower())\n",
    "print(\"Herbarium images for unpaired classes in train_remapped.txt:\", count_herb_unpaired)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22055,
     "status": "ok",
     "timestamp": 1764140078657,
     "user": {
      "displayName": "Esther Chai",
      "userId": "05451503381207659663"
     },
     "user_tz": -480
    },
    "id": "v3HGEw_BytRQ",
    "outputId": "011068bc-eb1d-43cf-cec6-058593d3bd9a"
   },
   "outputs": [],
   "source": [
    "# Clean folders\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "CHECK_DIR = Path(\"contrastive-unpaired-translation/checkpoints/herb2field_cut_fast\")\n",
    "RESULT_DIR = Path(\"/content/drive/MyDrive/COS30082_Cross_Domain/models/fastcut_results/herb2field_cut_fast\")\n",
    "\n",
    "shutil.rmtree(CHECK_DIR, ignore_errors=True)\n",
    "shutil.rmtree(RESULT_DIR, ignore_errors=True)\n",
    "\n",
    "print(\"Cleaned old checkpoints + results.\")\n",
    "\n",
    "shutil.rmtree(FASTCUT_ROOT, ignore_errors=True)\n",
    "FASTCUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "print(\"Cleaned FASTCUT data directories.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "executionInfo": {
     "elapsed": 342345,
     "status": "error",
     "timestamp": 1764140421021,
     "user": {
      "displayName": "Esther Chai",
      "userId": "05451503381207659663"
     },
     "user_tz": -480
    },
    "id": "AZQw55gRIBf_",
    "outputId": "b3fff42f-f037-4a5e-de82-c6ccc2825a8f"
   },
   "outputs": [],
   "source": [
    "# Block 5 - prepare FASTCUT directories (trainA/trainB/testA)\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "FASTCUT_ROOT = Path(FASTCUT_WORKDIR) / \"fastcut_data\"\n",
    "trainA = FASTCUT_ROOT / \"trainA\"\n",
    "trainB = FASTCUT_ROOT / \"trainB\"\n",
    "valA = FASTCUT_ROOT / \"valA\"\n",
    "valB = FASTCUT_ROOT / \"valB\"\n",
    "testA = FASTCUT_ROOT / \"testA\"\n",
    "\n",
    "# reset directories\n",
    "for p in (trainA, trainB, valA, valB, testA):\n",
    "    if p.exists():\n",
    "        shutil.rmtree(p)\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Build trainA/trainB from your remapped train list\n",
    "addedA = 0\n",
    "addedB = 0\n",
    "\n",
    "with open(TRAIN_REMAP, \"r\", encoding=\"utf-8\") as f:\n",
    "    for ln in f:\n",
    "        ln = ln.strip()\n",
    "        if not ln: continue\n",
    "        rel, cls_str = ln.split()\n",
    "        cls = int(cls_str)\n",
    "        src = Path(DATA_ROOT) / rel\n",
    "        if not src.exists():\n",
    "            continue\n",
    "        if \"herbarium\" in rel.lower():\n",
    "            shutil.copy(src, trainA / src.name)\n",
    "            addedA += 1\n",
    "        elif \"photo\" in rel.lower() or \"field\" in rel.lower():\n",
    "            shutil.copy(src, trainB / src.name)\n",
    "            addedB += 1\n",
    "\n",
    "# Build testA using herbarium TRAIN images for UNPAIRED classes (index-based mapping needs these)\n",
    "added_testA = 0\n",
    "with open(TRAIN_REMAP, \"r\", encoding=\"utf-8\") as f:\n",
    "    for ln in f:\n",
    "        ln = ln.strip()\n",
    "        if not ln: continue\n",
    "        rel, cls_str = ln.split()\n",
    "        cls = int(cls_str)\n",
    "        if cls not in unpaired_new_ids:\n",
    "            continue\n",
    "        if \"herbarium\" not in rel.lower():\n",
    "            continue\n",
    "        src = Path(DATA_ROOT) / rel\n",
    "        if src.exists():\n",
    "            shutil.copy(src, testA / src.name)\n",
    "            added_testA += 1\n",
    "\n",
    "print(\"TrainA (herbarium) count:\", addedA)\n",
    "print(\"TrainB (photo/field) count:\", addedB)\n",
    "print(\"TestA (herbarium - unpaired candidate) count:\", added_testA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 50,
     "status": "aborted",
     "timestamp": 1764140421064,
     "user": {
      "displayName": "Esther Chai",
      "userId": "05451503381207659663"
     },
     "user_tz": -480
    },
    "id": "-C9el9EFIC9s"
   },
   "outputs": [],
   "source": [
    "# Block 6 - Train FastCUT (Windows Fixed - Corrected Arguments)\n",
    "\n",
    "# 1. Enter the directory\n",
    "%cd contrastive-unpaired-translation\n",
    "\n",
    "# 2. Install dependencies (if not already done)\n",
    "%pip install dominate visdom\n",
    "\n",
    "# 3. Run training\n",
    "# Changes made:\n",
    "# - Removed backslashes (\\) to fit on one line for Windows stability\n",
    "# - Changed --n_threads 0 to --num_threads 0 (this was the error)\n",
    "# - Kept -u for unbuffered output so you can see the progress bar\n",
    "!python -u train.py --dataroot \"{FASTCUT_ROOT}\" --name herb2field_cut_fast --model cut --no_dropout --gpu_ids {GPU_ID} --n_epochs 10 --n_epochs_decay 10 --batch_size 4 --save_epoch_freq 10 --print_freq 200 --load_size 192 --crop_size 128 --nce_layers 4,8 --num_patches 128 --no_html --display_id -1 --num_threads 0\n",
    "\n",
    "# 4. Go back to original directory\n",
    "%cd -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "aborted",
     "timestamp": 1764140421076,
     "user": {
      "displayName": "Esther Chai",
      "userId": "05451503381207659663"
     },
     "user_tz": -480
    },
    "id": "iFztOLh0pndx"
   },
   "outputs": [],
   "source": [
    "print(os.listdir(\"contrastive-unpaired-translation/checkpoints/herb2field_cut_fast\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "aborted",
     "timestamp": 1764140421080,
     "user": {
      "displayName": "Esther Chai",
      "userId": "05451503381207659663"
     },
     "user_tz": -480
    },
    "id": "SNRllviHprHx"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(f\"Listing all files in: {FASTCUT_ROOT}\")\n",
    "\n",
    "# os.walk allows us to look recursively into all subfolders (like ls -R)\n",
    "for root, dirs, files in os.walk(FASTCUT_ROOT):\n",
    "    # Print the current directory\n",
    "    print(f\"\\nðŸ“‚ {root}\")\n",
    "    # Print files in this directory\n",
    "    for file in files:\n",
    "        print(f\"   â””â”€â”€ {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 421211,
     "status": "aborted",
     "timestamp": 1764140421084,
     "user": {
      "displayName": "Esther Chai",
      "userId": "05451503381207659663"
     },
     "user_tz": -480
    },
    "id": "KdhY6-zGr0Ct"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"FASTCUT_ROOT\"] = str(FASTCUT_ROOT)\n",
    "FASTCUT_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "aborted",
     "timestamp": 1764140421092,
     "user": {
      "displayName": "Esther Chai",
      "userId": "05451503381207659663"
     },
     "user_tz": -480
    },
    "id": "S1FhAwPmZ3Xt"
   },
   "outputs": [],
   "source": [
    "# Block - Setup testB with a dummy image (Python Version)\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure FASTCUT_ROOT is a Path object\n",
    "root = Path(FASTCUT_ROOT)\n",
    "trainB = root / \"trainB\"\n",
    "testB = root / \"testB\"\n",
    "\n",
    "print(f\"FASTCUT_ROOT = {root}\")\n",
    "\n",
    "# 1. Create testB directory\n",
    "testB.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 2. Pick one field image from trainB\n",
    "# We use glob to find files, equivalent to 'ls'\n",
    "try:\n",
    "    # Get the first file found in trainB\n",
    "    first_field_img = next(trainB.glob(\"*\")) \n",
    "    print(f\"Dummy image source: {first_field_img.name}\")\n",
    "\n",
    "    # 3. Copy dummy image\n",
    "    dest = testB / \"dummy.jpg\"\n",
    "    shutil.copy(first_field_img, dest)\n",
    "    print(f\"Successfully copied to: {dest}\")\n",
    "\n",
    "    # 4. Verify (List files in testB)\n",
    "    print(\"Contents of testB:\", list(testB.glob(\"*\")))\n",
    "\n",
    "except StopIteration:\n",
    "    print(\"âŒ Error: No images found in trainB! Make sure Block 5 ran correctly.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"âŒ Error: Directory not found: {trainB}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "aborted",
     "timestamp": 1764140421098,
     "user": {
      "displayName": "Esther Chai",
      "userId": "05451503381207659663"
     },
     "user_tz": -480
    },
    "id": "8UgsgYYMIIaj"
   },
   "outputs": [],
   "source": [
    "# Block 7 - FastCUT inference (Windows Fixed)\n",
    "\n",
    "# 1. Enter the directory\n",
    "%cd contrastive-unpaired-translation\n",
    "\n",
    "# 2. Run inference on a SINGLE LINE\n",
    "# Changes:\n",
    "# - Combined into one line to fix the {GPU_ID} error\n",
    "# - Changed {DRIVE_ROOT} to {PROJECT_ROOT} so it saves to your local hard drive correctly\n",
    "!python test.py --dataroot \"{FASTCUT_ROOT}\" --name herb2field_cut_fast --model cut --no_dropout --phase test --serial_batches --results_dir \"{PROJECT_ROOT}/models/fastcut_results\" --num_test 999999 --gpu_ids {GPU_ID}\n",
    "\n",
    "# 3. Return to project root\n",
    "%cd -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "aborted",
     "timestamp": 1764140421106,
     "user": {
      "displayName": "Esther Chai",
      "userId": "05451503381207659663"
     },
     "user_tz": -480
    },
    "id": "ABB0WWUhBaVy"
   },
   "outputs": [],
   "source": [
    "# verify\n",
    "from pathlib import Path\n",
    "fakeB = Path(PROJECT_ROOT) / \"models/fastcut_results/herb2field_cut_fast/test_latest/images/fake_B\"\n",
    "print(\"fake_B exists:\", fakeB.exists())\n",
    "print(\"png count:\", len(list(fakeB.glob(\"*.png\"))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 421214,
     "status": "aborted",
     "timestamp": 1764140421109,
     "user": {
      "displayName": "Esther Chai",
      "userId": "05451503381207659663"
     },
     "user_tz": -480
    },
    "id": "Gi-q4m6gINFL"
   },
   "outputs": [],
   "source": [
    "# Block 9 - Integrate synthetic images via SAFE NUMERIC MATCHING\n",
    "\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "# Paths\n",
    "SYN_SRC_FAKEB = Path(PROJECT_ROOT) / \"models/fastcut_results/herb2field_cut_fast/test_latest/images/fake_B\"\n",
    "FASTCUT_TESTA = FASTCUT_ROOT / \"testA\"\n",
    "OUTPUT_LIST = TRAIN_WITH_SYN\n",
    "SYN_DEST_DIR = Path(DATA_ROOT) / \"synthetic\"\n",
    "SYN_DEST_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ------- Helper: extract numeric ID from filenames --------\n",
    "def extract_id(path_obj):\n",
    "    \"\"\"extract numeric ID from basename, removes extension\"\"\"\n",
    "    s = path_obj.stem              # e.g. '10028' or '10028_fake'\n",
    "    nums = re.findall(r'\\d+', s)\n",
    "    return int(nums[-1]) if nums else -1\n",
    "\n",
    "# ------- Load and sort testA and fakeB numerically --------\n",
    "testA_files = sorted(\n",
    "    [p for p in FASTCUT_TESTA.iterdir() if p.is_file()],\n",
    "    key=extract_id\n",
    ")\n",
    "\n",
    "fakeB_files = sorted(\n",
    "    [p for p in SYN_SRC_FAKEB.iterdir() if p.suffix.lower() in [\".png\", \".jpg\", \".jpeg\"]],\n",
    "    key=extract_id\n",
    ")\n",
    "\n",
    "print(\"testA count:\", len(testA_files))\n",
    "print(\"fakeB count:\", len(fakeB_files))\n",
    "\n",
    "if len(testA_files) != len(fakeB_files):\n",
    "    print(\"âš  WARNING: testA and fakeB counts differ. Using min() to match.\")\n",
    "n = min(len(testA_files), len(fakeB_files))\n",
    "print(\"Mapping count:\", n)\n",
    "\n",
    "# ------- Load original remap list --------\n",
    "with open(TRAIN_REMAP, \"r\", encoding=\"utf-8\") as f:\n",
    "    orig_train_lines = [ln.strip() for ln in f if ln.strip()]\n",
    "\n",
    "# Build lookup from train_remap filename to label\n",
    "testA_name_to_label = {}\n",
    "for ln in orig_train_lines:\n",
    "    parts = ln.split()\n",
    "    if len(parts) < 2:\n",
    "        continue\n",
    "    rel = parts[0]\n",
    "    cls = int(parts[1])\n",
    "    fname = Path(rel).name\n",
    "    if fname not in testA_name_to_label:\n",
    "        testA_name_to_label[fname] = cls\n",
    "\n",
    "print(\"Indexed train_remap entries:\", len(testA_name_to_label))\n",
    "\n",
    "# ------- NEW list for training with synthetic --------\n",
    "new_lines = orig_train_lines.copy()\n",
    "added = 0\n",
    "missing_match = 0\n",
    "examples = []\n",
    "\n",
    "for i in range(n):\n",
    "    testA_path = testA_files[i]\n",
    "    fakeB_path = fakeB_files[i]\n",
    "\n",
    "    # match by numeric ID\n",
    "    id_A = extract_id(testA_path)\n",
    "    id_B = extract_id(fakeB_path)\n",
    "\n",
    "    if id_A != id_B:\n",
    "        print(f\"âš  ID mismatch: {testA_path.name} != {fakeB_path.name}, skipping.\")\n",
    "        missing_match += 1\n",
    "        continue\n",
    "\n",
    "    # lookup label from original train remap\n",
    "    testA_name = testA_path.name\n",
    "    if testA_name not in testA_name_to_label:\n",
    "        missing_match += 1\n",
    "        print(\"âš  No label found for\", testA_name)\n",
    "        continue\n",
    "\n",
    "    cls = testA_name_to_label[testA_name]\n",
    "\n",
    "    # Add only if class is unpaired\n",
    "    if cls not in unpaired_new_ids:\n",
    "        continue\n",
    "\n",
    "    # Copy fake image to synthetic folder\n",
    "    dest = SYN_DEST_DIR / fakeB_path.name\n",
    "    if not dest.exists():\n",
    "        shutil.copy(fakeB_path, dest)\n",
    "\n",
    "    # Append synthetic entry\n",
    "    syn_rel = f\"synthetic/{fakeB_path.name}\"\n",
    "    new_lines.append(f\"{syn_rel} {cls}\")\n",
    "    added += 1\n",
    "\n",
    "    if len(examples) < 5:\n",
    "        examples.append((testA_path.name, fakeB_path.name, cls))\n",
    "\n",
    "print(\"âœ¨ Synthetic integration complete!\")\n",
    "print(\"Added synthetic images:\", added)\n",
    "print(\"Missing matches:\", missing_match)\n",
    "print(\"Examples:\", examples)\n",
    "\n",
    "# ------- Save new training list --------\n",
    "with open(OUTPUT_LIST, \"w\", encoding=\"utf-8\") as f:\n",
    "    for ln in new_lines:\n",
    "        f.write(ln + \"\\n\")\n",
    "\n",
    "print(\"ðŸ“ Saved new train-with-synth file:\", OUTPUT_LIST)\n",
    "print(\"Total training lines:\", len(new_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 421215,
     "status": "aborted",
     "timestamp": 1764140421114,
     "user": {
      "displayName": "Esther Chai",
      "userId": "05451503381207659663"
     },
     "user_tz": -480
    },
    "id": "TjQu1mZeO7DO"
   },
   "outputs": [],
   "source": [
    "# === Block X: Build validation list using test.txt + groundtruth.txt ===\n",
    "from pathlib import Path\n",
    "\n",
    "TEST_LIST = Path(DATA_ROOT) / \"list/test.txt\"\n",
    "GROUNDTRUTH = Path(DATA_ROOT) / \"list/groundtruth.txt\"\n",
    "VAL_OUT = REMAPPED_DIR / \"test_remapped_fixed.txt\"\n",
    "\n",
    "# sanity checks\n",
    "if not TEST_LIST.exists():\n",
    "    raise FileNotFoundError(\"Missing test.txt at: \" + str(TEST_LIST))\n",
    "if not GROUNDTRUTH.exists():\n",
    "    raise FileNotFoundError(\"Missing groundtruth.txt at: \" + str(GROUNDTRUTH))\n",
    "\n",
    "# Step 1: Load test list (paths only)\n",
    "test_paths = []\n",
    "with open(TEST_LIST, \"r\", encoding=\"utf-8\") as f:\n",
    "    for ln in f:\n",
    "        ln = ln.strip()\n",
    "        if ln:\n",
    "            test_paths.append(ln)\n",
    "\n",
    "print(\"Found test images:\", len(test_paths))\n",
    "\n",
    "# Step 2: Load groundtruth mapping: path â†’ original class ID\n",
    "gt_map = {}\n",
    "with open(GROUNDTRUTH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for ln in f:\n",
    "        ln = ln.strip()\n",
    "        if not ln: continue\n",
    "        parts = ln.split()\n",
    "        if len(parts) != 2: continue\n",
    "        rel, orig_lab = parts[0], int(parts[1])\n",
    "        gt_map[rel] = orig_lab\n",
    "\n",
    "print(\"Groundtruth entries:\", len(gt_map))\n",
    "\n",
    "# Step 3: Build final validation list with new indices\n",
    "missing = 0\n",
    "written = 0\n",
    "with open(VAL_OUT, \"w\", encoding=\"utf-8\") as f_out:\n",
    "    for rel in test_paths:\n",
    "        if rel not in gt_map:\n",
    "            missing += 1\n",
    "            continue\n",
    "\n",
    "        orig_lab = gt_map[rel]\n",
    "\n",
    "        # map using orig_to_new\n",
    "        if orig_lab not in orig_to_new:\n",
    "            missing += 1\n",
    "            continue\n",
    "\n",
    "        new_lab = orig_to_new[orig_lab]\n",
    "        f_out.write(f\"{rel} {new_lab}\\n\")\n",
    "        written += 1\n",
    "\n",
    "print(\"\\n=== Validation file built ===\")\n",
    "print(\"Written lines:\", written)\n",
    "print(\"Missing gt or unmapped:\", missing)\n",
    "print(\"Saved to:\", VAL_OUT)\n",
    "\n",
    "# Step 4: Sanity check â€” ensure all labels are 0â€“99\n",
    "bad = []\n",
    "with open(VAL_OUT, \"r\") as f:\n",
    "    for i, ln in enumerate(f):\n",
    "        parts = ln.strip().split()\n",
    "        if len(parts) != 2: continue\n",
    "        lab = int(parts[1])\n",
    "        if lab < 0 or lab >= 100:\n",
    "            bad.append((i, lab, ln.strip()))\n",
    "\n",
    "print(\"Bad labels remaining:\", len(bad))\n",
    "print(bad[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 421232,
     "status": "aborted",
     "timestamp": 1764140421135,
     "user": {
      "displayName": "Esther Chai",
      "userId": "05451503381207659663"
     },
     "user_tz": -480
    },
    "id": "QwlApDzGIOjO"
   },
   "outputs": [],
   "source": [
    "# Block 10 - Dataset class and dataloaders\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "\n",
    "class SimpleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, list_path, transform=None, expect_domain=False):\n",
    "        self.root = Path(root)\n",
    "        self.transform = transform\n",
    "        self.expect_domain = expect_domain\n",
    "        self.samples = []\n",
    "        with open(list_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for ln in f:\n",
    "                ln = ln.strip()\n",
    "                if not ln: continue\n",
    "                parts = ln.split()\n",
    "                rel = parts[0]\n",
    "                cls = int(parts[1]) if len(parts) > 1 else -1\n",
    "                dom = int(parts[2]) if len(parts) > 2 else 0\n",
    "                self.samples.append((self.root / rel, cls, dom))\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        path, cls, dom = self.samples[idx]\n",
    "        try:\n",
    "            img = Image.open(path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            # return None to be cleaned by collate\n",
    "            # also print once for debugging\n",
    "            print(f\"[WARN] failed to open {path}: {e}\")\n",
    "            return None\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, cls, dom\n",
    "\n",
    "def clean_collate(batch):\n",
    "    batch = [b for b in batch if b is not None]\n",
    "    if not batch:\n",
    "        return None\n",
    "    return torch.utils.data.default_collate(batch)\n",
    "\n",
    "# transforms (match backbone input; adjust sizes as needed)\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.Resize((518,518)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "val_tf = transforms.Compose([\n",
    "    transforms.Resize((518,518)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "# Build datasets using the new remapped list\n",
    "train_ds = SimpleDataset(DATA_ROOT, str(OUTPUT_LIST), transform=train_tf, expect_domain=False)\n",
    "val_ds = SimpleDataset(DATA_ROOT, str(REMAPPED_DIR / \"test_remapped_fixed.txt\"), transform=val_tf, expect_domain=False)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, collate_fn=clean_collate)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, collate_fn=clean_collate)\n",
    "\n",
    "print(\"Train size:\", len(train_ds), \"Val size:\", len(val_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 421231,
     "status": "aborted",
     "timestamp": 1764140421138,
     "user": {
      "displayName": "Esther Chai",
      "userId": "05451503381207659663"
     },
     "user_tz": -480
    },
    "id": "f1W36UcJIQNB"
   },
   "outputs": [],
   "source": [
    "# Block 11 - Load Dinov2 backbone checkpoint and build a wrapper\n",
    "import timm\n",
    "import torch.nn as nn\n",
    "\n",
    "BACKBONE_CHECKPOINT = f\"{PROJECT_ROOT}/model_best.pth.tar\"  # edit if different\n",
    "if not Path(BACKBONE_CHECKPOINT).exists():\n",
    "    print(\"Warning: backbone checkpoint not found at\", BACKBONE_CHECKPOINT)\n",
    "print(\"Attempting to create backbone model...\")\n",
    "\n",
    "backbone = timm.create_model(\"vit_base_patch14_reg4_dinov2.lvd142m\", pretrained=False, num_classes=0)\n",
    "feat_dim = getattr(backbone, \"num_features\", None) or getattr(backbone, \"embed_dim\", 768)\n",
    "print(\"Feature dim:\", feat_dim)\n",
    "\n",
    "class DINOWrapper(nn.Module):\n",
    "    def __init__(self, backbone, feat_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.head = nn.Linear(feat_dim, num_classes)\n",
    "    def forward(self, x, return_feat=False):\n",
    "        feats = self.backbone(x)  # (B, feat_dim)\n",
    "        if return_feat:\n",
    "            return feats\n",
    "        logits = self.head(feats)\n",
    "        return logits\n",
    "\n",
    "NUM_CLASSES = 100\n",
    "model = DINOWrapper(backbone, feat_dim, NUM_CLASSES)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(DEVICE)\n",
    "print(\"Model created. Device:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 421230,
     "status": "aborted",
     "timestamp": 1764140421140,
     "user": {
      "displayName": "Esther Chai",
      "userId": "05451503381207659663"
     },
     "user_tz": -480
    },
    "id": "RbmvKQXp9oWc"
   },
   "outputs": [],
   "source": [
    "model_cpu = model.to(\"cpu\")\n",
    "x = torch.randn(1, 3, 518, 518)\n",
    "out = model_cpu(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 421228,
     "status": "aborted",
     "timestamp": 1764140421143,
     "user": {
      "displayName": "Esther Chai",
      "userId": "05451503381207659663"
     },
     "user_tz": -480
    },
    "id": "QNeXpl2iIRP8"
   },
   "outputs": [],
   "source": [
    "# Block 12 - permissive checkpoint load (to backbone)\n",
    "import torch\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "\n",
    "ckpt_path = Path(BACKBONE_CHECKPOINT)\n",
    "if ckpt_path.exists():\n",
    "    # allow argparse.Namespace types if checkpoint contains them\n",
    "    torch.serialization.add_safe_globals([argparse.Namespace])\n",
    "    ckpt = torch.load(str(ckpt_path), map_location=\"cpu\", weights_only=False)\n",
    "    # find state dict\n",
    "    if \"state_dict\" in ckpt:\n",
    "        state = ckpt[\"state_dict\"]\n",
    "    else:\n",
    "        state = ckpt\n",
    "    # clean keys\n",
    "    new_state = {}\n",
    "    for k,v in state.items():\n",
    "        nk = k.replace(\"module.\", \"\")\n",
    "        # drop leading \"backbone.\" if present\n",
    "        if nk.startswith(\"backbone.\"):\n",
    "            nk2 = nk.replace(\"backbone.\",\"\")\n",
    "            new_state[nk2] = v\n",
    "        else:\n",
    "            new_state[nk] = v\n",
    "    # try load into backbone\n",
    "    missing, unexpected = model.backbone.load_state_dict(new_state, strict=False)\n",
    "    print(\"Backbone load missing:\", len(missing), \"unexpected:\", len(unexpected))\n",
    "else:\n",
    "    print(\"Checkpoint not found, continuing with random init for backbone.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 421245,
     "status": "aborted",
     "timestamp": 1764140421163,
     "user": {
      "displayName": "Esther Chai",
      "userId": "05451503381207659663"
     },
     "user_tz": -480
    },
    "id": "sybL0lFvKfBw"
   },
   "outputs": [],
   "source": [
    "# ===== Block X : Build paired_set and unpaired_set =====\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=== Building paired and unpaired sets ===\")\n",
    "\n",
    "# Path to unpaired species (original IDs, as provided)\n",
    "UNPAIRED_PATH = PROJECT_ROOT / \"AML_project_herbarium_dataset/list/class_without_pairs.txt\"\n",
    "\n",
    "# Load original unpaired IDs\n",
    "unpaired_orig_ids = []\n",
    "with open(UNPAIRED_PATH, \"r\") as f:\n",
    "    for ln in f:\n",
    "        ln = ln.strip()\n",
    "        if ln.isdigit():\n",
    "            unpaired_orig_ids.append(int(ln))\n",
    "\n",
    "print(\"Unpaired ORIGINAL species count:\", len(unpaired_orig_ids))\n",
    "\n",
    "# Convert original IDs â†’ new 0..99 mapped IDs\n",
    "unpaired_set = set()\n",
    "for oid in unpaired_orig_ids:\n",
    "    if oid in orig_to_new:\n",
    "        unpaired_set.add(orig_to_new[oid])\n",
    "    else:\n",
    "        print(\"WARNING: original ID not in mapping:\", oid)\n",
    "\n",
    "print(\"Unpaired mapped count:\", len(unpaired_set))\n",
    "\n",
    "# Paired set = all classes except unpaired ones\n",
    "all_classes = set(range(100))\n",
    "paired_set = all_classes - unpaired_set\n",
    "\n",
    "print(\"Paired count:\", len(paired_set))\n",
    "\n",
    "print(\"Sample unpaired mapped:\", sorted(list(unpaired_set))[:10])\n",
    "print(\"Sample paired mapped:\", sorted(list(paired_set))[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 421246,
     "status": "aborted",
     "timestamp": 1764140421165,
     "user": {
      "displayName": "Esther Chai",
      "userId": "05451503381207659663"
     },
     "user_tz": -480
    },
    "id": "5UWDRAs7T8My"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 421244,
     "status": "aborted",
     "timestamp": 1764140421168,
     "user": {
      "displayName": "Esther Chai",
      "userId": "05451503381207659663"
     },
     "user_tz": -480
    },
    "id": "DvbEMM73ISRM"
   },
   "outputs": [],
   "source": [
    "# ===== Block 13 : Head-only + Fine-tune training + dual checkpoint saving =====\n",
    "import torch, torch.optim as optim, torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "# %pip install matplotlib # <--- Commented out to save time if already installed\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Config (tune these) ---\n",
    "EPOCHS_HEAD = 10       # head only\n",
    "EPOCHS_FINE = 10       # fine-tune\n",
    "LR_HEAD = 1e-4\n",
    "LR_FINE = 1e-5\n",
    "WEIGHT_DECAY = 1e-4\n",
    "BATCH_SIZE = 4\n",
    "SAVE_DIR = PROJECT_ROOT / \"models\"\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "best_overall_path = SAVE_DIR / \"best_overall.pth\"\n",
    "best_unpaired_path = SAVE_DIR / \"best_unpaired.pth\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# ==============================================================================\n",
    "# ðŸ”´ FIX: Move model to GPU explicitly before creating optimizer\n",
    "# This fixes the \"Input type (cuda) and weight type (cpu) should be the same\" error\n",
    "model = model.to(device)\n",
    "# ==============================================================================\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# -- helpers: compute paired/unpaired accuracy on a loader\n",
    "@torch.no_grad()\n",
    "def eval_paired_unpaired(model, loader):\n",
    "    model.eval()\n",
    "    paired_correct = paired_total = 0\n",
    "    unpaired_correct = unpaired_total = 0\n",
    "    for batch in loader:\n",
    "        # allow val loader that returns (imgs, labels) or (imgs, labels, domain)\n",
    "        if isinstance(batch, (list, tuple)) and len(batch) >= 2:\n",
    "            imgs, labels = batch[0], batch[1]\n",
    "        else:\n",
    "            raise RuntimeError(\"Val loader must return at least (imgs, labels)\")\n",
    "        imgs = imgs.to(device); labels = labels.to(device)\n",
    "        logits = model(imgs)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        for lab, pred in zip(labels, preds):\n",
    "            lab_i = int(lab.item())\n",
    "            if lab_i in paired_set:\n",
    "                paired_total += 1\n",
    "                if int(pred.item()) == lab_i:\n",
    "                    paired_correct += 1\n",
    "            else:\n",
    "                unpaired_total += 1\n",
    "                if int(pred.item()) == lab_i:\n",
    "                    unpaired_correct += 1\n",
    "\n",
    "    paired_acc = 100.0 * paired_correct / max(1, paired_total)\n",
    "    unpaired_acc = 100.0 * unpaired_correct / max(1, unpaired_total)\n",
    "    return paired_acc, unpaired_acc\n",
    "\n",
    "# -- one epoch train (standard)\n",
    "def train_one_epoch(model, loader, optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    pbar = tqdm(loader, desc=\"train\", leave=False)\n",
    "    for batch in pbar:\n",
    "        if isinstance(batch, (list,tuple)) and len(batch) >= 2:\n",
    "            imgs, labels = batch[0], batch[1]\n",
    "        else:\n",
    "            raise RuntimeError(\"Train loader must return (imgs, labels[,domain])\")\n",
    "        imgs = imgs.to(device); labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(imgs)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        bs = imgs.size(0)\n",
    "        running_loss += loss.item() * bs\n",
    "        correct += (logits.argmax(1) == labels).sum().item()\n",
    "        total += bs\n",
    "        pbar.set_postfix({\"loss\": running_loss / total})\n",
    "    return running_loss / max(1,total), 100.0 * correct / max(1,total)\n",
    "\n",
    "# -- validation that returns loss + overall Top1% (for monitoring)\n",
    "@torch.no_grad()\n",
    "def validate_overall(model, loader):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in loader:\n",
    "        if isinstance(batch, (list,tuple)) and len(batch) >= 2:\n",
    "            imgs, labels = batch[0], batch[1]\n",
    "        else:\n",
    "            raise RuntimeError(\"Val loader must return (imgs, labels[,domain])\")\n",
    "        imgs = imgs.to(device); labels = labels.to(device)\n",
    "        logits = model(imgs)\n",
    "        loss = criterion(logits, labels)\n",
    "        bs = imgs.size(0)\n",
    "        running_loss += loss.item() * bs\n",
    "        correct += (logits.argmax(1) == labels).sum().item()\n",
    "        total += bs\n",
    "    return running_loss / max(1,total), 100.0 * correct / max(1,total)\n",
    "\n",
    "# --- Storage for curves\n",
    "train_losses = []; train_accs = []\n",
    "val_losses = []; val_accs = []\n",
    "paired_curve = []; unpaired_curve = []\n",
    "\n",
    "# --- Freeze backbone (head-only) ---\n",
    "for p in model.backbone.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "optim_head = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                         lr=LR_HEAD, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "best_overall = 0.0\n",
    "best_unpaired = 0.0\n",
    "\n",
    "print(\"==== HEAD-ONLY TRAINING ====\")\n",
    "for ep in range(1, EPOCHS_HEAD+1):\n",
    "    t_loss, t_acc = train_one_epoch(model, train_loader, optim_head)\n",
    "    v_loss, v_acc = validate_overall(model, val_loader)\n",
    "    p_acc, u_acc = eval_paired_unpaired(model, val_loader)\n",
    "\n",
    "    train_losses.append(t_loss); train_accs.append(t_acc)\n",
    "    val_losses.append(v_loss); val_accs.append(v_acc)\n",
    "    paired_curve.append(p_acc); unpaired_curve.append(u_acc)\n",
    "\n",
    "    print(f\"[HEAD] Epoch {ep}/{EPOCHS_HEAD} | Train loss {t_loss:.4f} Train Top1 {t_acc:.2f}%\")\n",
    "    print(f\"       Val loss {v_loss:.4f} Val Top1 {v_acc:.2f}% | Paired {p_acc:.2f}% Unpaired {u_acc:.2f}%\")\n",
    "\n",
    "    # save best overall\n",
    "    if v_acc > best_overall:\n",
    "        best_overall = v_acc\n",
    "        torch.save({\n",
    "            \"epoch\": ep,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"val_top1\": v_acc,\n",
    "            \"unpaired_top1\": u_acc\n",
    "        }, best_overall_path)\n",
    "        print(\"ðŸ’¾ Saved best_overall ->\", best_overall_path)\n",
    "\n",
    "    # save best unpaired\n",
    "    if u_acc > best_unpaired:\n",
    "        best_unpaired = u_acc\n",
    "        torch.save({\n",
    "            \"epoch\": ep,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"val_top1\": v_acc,\n",
    "            \"unpaired_top1\": u_acc\n",
    "        }, best_unpaired_path)\n",
    "        print(\"ðŸ’¾ Saved best_unpaired ->\", best_unpaired_path)\n",
    "\n",
    "# --- Unfreeze backbone and fine-tune all weights ---\n",
    "for p in model.backbone.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "optim_fine = optim.AdamW(model.parameters(), lr=LR_FINE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "print(\"\\n==== FINE-TUNE FULL MODEL ====\")\n",
    "for ep in range(1, EPOCHS_FINE+1):\n",
    "    t_loss, t_acc = train_one_epoch(model, train_loader, optim_fine)\n",
    "    v_loss, v_acc = validate_overall(model, val_loader)\n",
    "    p_acc, u_acc = eval_paired_unpaired(model, val_loader)\n",
    "\n",
    "    train_losses.append(t_loss); train_accs.append(t_acc)\n",
    "    val_losses.append(v_loss); val_accs.append(v_acc)\n",
    "    paired_curve.append(p_acc); unpaired_curve.append(u_acc)\n",
    "\n",
    "    epoch_index = EPOCHS_HEAD + ep\n",
    "    print(f\"[FINE] Epoch {epoch_index}/{EPOCHS_HEAD+EPOCHS_FINE} | Train loss {t_loss:.4f} Train Top1 {t_acc:.2f}%\")\n",
    "    print(f\"       Val loss {v_loss:.4f} Val Top1 {v_acc:.2f}% | Paired {p_acc:.2f}% Unpaired {u_acc:.2f}%\")\n",
    "\n",
    "    if v_acc > best_overall:\n",
    "        best_overall = v_acc\n",
    "        torch.save({\n",
    "            \"epoch\": epoch_index,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"val_top1\": v_acc,\n",
    "            \"unpaired_top1\": u_acc\n",
    "        }, best_overall_path)\n",
    "        print(\"ðŸ’¾ Saved best_overall ->\", best_overall_path)\n",
    "\n",
    "    if u_acc > best_unpaired:\n",
    "        best_unpaired = u_acc\n",
    "        torch.save({\n",
    "            \"epoch\": epoch_index,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"val_top1\": v_acc,\n",
    "            \"unpaired_top1\": u_acc\n",
    "        }, best_unpaired_path)\n",
    "        print(\"ðŸ’¾ Saved best_unpaired ->\", best_unpaired_path)\n",
    "\n",
    "# --- Final plots ---\n",
    "plt.figure(figsize=(14,5))\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(val_losses, label=\"Val Loss\")\n",
    "plt.title(\"Loss\"); plt.legend(); plt.grid()\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(train_accs, label=\"Train Top1\")\n",
    "plt.plot(val_accs, label=\"Val Top1\")\n",
    "plt.title(\"Top-1 Acc\"); plt.legend(); plt.grid()\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.plot(paired_curve, label=\"Paired Top1\")\n",
    "plt.plot(unpaired_curve, label=\"Unpaired Top1\")\n",
    "plt.title(\"Paired / Unpaired\"); plt.legend(); plt.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Done. Best overall Top1:\", best_overall, \" Best unpaired Top1:\", best_unpaired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Block 13 : RESUME MODE (Skip Head-Only if checkpoint exists) =====\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Config ---\n",
    "EPOCHS_HEAD = 10       \n",
    "EPOCHS_FINE = 10       \n",
    "LR_HEAD = 1e-4\n",
    "LR_FINE = 5e-5  # Increased slightly to help DINOv2 adapt faster\n",
    "WEIGHT_DECAY = 1e-4\n",
    "SAVE_DIR = PROJECT_ROOT / \"models\"\n",
    "BATCH_SIZE = 4\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "best_overall_path = SAVE_DIR / \"best_overall.pth\"\n",
    "best_unpaired_path = SAVE_DIR / \"best_unpaired.pth\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# ðŸ”´ CRITICAL FIX: Move model to GPU explicitly before optimizing\n",
    "model = model.to(device)\n",
    "\n",
    "# ============================================================\n",
    "# ðŸŸ¢ RESUME LOGIC\n",
    "# ============================================================\n",
    "run_head_training = True\n",
    "start_epoch_fine = 1\n",
    "best_overall = 0.0\n",
    "best_unpaired = 0.0\n",
    "\n",
    "# Arrays to store metrics (will be empty if resuming, but that's okay)\n",
    "train_losses, train_accs = [], []\n",
    "val_losses, val_accs = [], []\n",
    "paired_curve, unpaired_curve = [], []\n",
    "\n",
    "if best_overall_path.exists():\n",
    "    print(f\"Found saved checkpoint: {best_overall_path}\")\n",
    "    print(\"Loading weights...\")\n",
    "    checkpoint = torch.load(best_overall_path, map_location=device)\n",
    "    \n",
    "    # Load the state\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    best_overall = checkpoint.get('val_top1', 0.0)\n",
    "    best_unpaired = checkpoint.get('unpaired_top1', 0.0)\n",
    "    \n",
    "    print(f\"âœ… Weights loaded! Best Overall: {best_overall:.2f}%, Best Unpaired: {best_unpaired:.2f}%\")\n",
    "    print(\"Skipping Head-Only training phase.\")\n",
    "    run_head_training = False\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting from scratch.\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# --- Helper Functions (Required inside the block for standalone running) ---\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_paired_unpaired(model, loader):\n",
    "    model.eval()\n",
    "    paired_correct = paired_total = 0\n",
    "    unpaired_correct = unpaired_total = 0\n",
    "    for batch in loader:\n",
    "        if batch is None: continue\n",
    "        imgs, labels, dom = batch\n",
    "        imgs = imgs.to(device); labels = labels.to(device)\n",
    "        logits = model(imgs)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        for lab, pred in zip(labels, preds):\n",
    "            lab_i = int(lab.item())\n",
    "            if lab_i in unpaired_set:\n",
    "                unpaired_total += 1\n",
    "                if int(pred.item()) == lab_i:\n",
    "                    unpaired_correct += 1\n",
    "            else:\n",
    "                paired_total += 1\n",
    "                if int(pred.item()) == lab_i:\n",
    "                    paired_correct += 1\n",
    "    paired_acc = 100.0 * paired_correct / max(1, paired_total)\n",
    "    unpaired_acc = 100.0 * unpaired_correct / max(1, unpaired_total)\n",
    "    return paired_acc, unpaired_acc\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    pbar = tqdm(loader, desc=\"train\", leave=False)\n",
    "    for batch in pbar:\n",
    "        if batch is None: continue\n",
    "        imgs, labels, dom = batch\n",
    "        imgs = imgs.to(device); labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(imgs)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        bs = imgs.size(0)\n",
    "        running_loss += loss.item() * bs\n",
    "        correct += (logits.argmax(1) == labels).sum().item()\n",
    "        total += bs\n",
    "        pbar.set_postfix({\"loss\": running_loss / max(1, total)})\n",
    "        \n",
    "    return running_loss / max(1,total), 100.0 * correct / max(1,total)\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_overall(model, loader):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in loader:\n",
    "        if batch is None: continue\n",
    "        imgs, labels, dom = batch\n",
    "        imgs = imgs.to(device); labels = labels.to(device)\n",
    "        logits = model(imgs)\n",
    "        loss = criterion(logits, labels)\n",
    "        bs = imgs.size(0)\n",
    "        running_loss += loss.item() * bs\n",
    "        correct += (logits.argmax(1) == labels).sum().item()\n",
    "        total += bs\n",
    "    return running_loss / max(1,total), 100.0 * correct / max(1,total)\n",
    "\n",
    "# =========================================\n",
    "# PHASE 1: HEAD ONLY (Skipped if Resuming)\n",
    "# =========================================\n",
    "if run_head_training:\n",
    "    # Freeze backbone\n",
    "    for p in model.backbone.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    optim_head = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                             lr=LR_HEAD, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    print(\"\\n==== HEAD-ONLY TRAINING ====\")\n",
    "    for ep in range(1, EPOCHS_HEAD+1):\n",
    "        t_loss, t_acc = train_one_epoch(model, train_loader, optim_head)\n",
    "        v_loss, v_acc = validate_overall(model, val_loader)\n",
    "        p_acc, u_acc = eval_paired_unpaired(model, val_loader)\n",
    "\n",
    "        train_losses.append(t_loss); train_accs.append(t_acc)\n",
    "        val_losses.append(v_loss); val_accs.append(v_acc)\n",
    "        paired_curve.append(p_acc); unpaired_curve.append(u_acc)\n",
    "\n",
    "        print(f\"[HEAD] Epoch {ep}/{EPOCHS_HEAD} | Val Top1 {v_acc:.2f}% | Paired {p_acc:.2f}% Unpaired {u_acc:.2f}%\")\n",
    "\n",
    "        if v_acc > best_overall:\n",
    "            best_overall = v_acc\n",
    "            torch.save({\n",
    "                \"epoch\": ep,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"val_top1\": v_acc,\n",
    "                \"unpaired_top1\": u_acc\n",
    "            }, best_overall_path)\n",
    "            print(\"ðŸ’¾ Saved best_overall\")\n",
    "\n",
    "# =========================================\n",
    "# PHASE 2: FINE-TUNING (Always Runs)\n",
    "# =========================================\n",
    "# Unfreeze backbone\n",
    "for p in model.backbone.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "optim_fine = optim.AdamW(model.parameters(), lr=LR_FINE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "print(\"\\n==== FINE-TUNE FULL MODEL ====\")\n",
    "for ep in range(1, EPOCHS_FINE+1):\n",
    "    t_loss, t_acc = train_one_epoch(model, train_loader, optim_fine)\n",
    "    v_loss, v_acc = validate_overall(model, val_loader)\n",
    "    p_acc, u_acc = eval_paired_unpaired(model, val_loader)\n",
    "\n",
    "    train_losses.append(t_loss); train_accs.append(t_acc)\n",
    "    val_losses.append(v_loss); val_accs.append(v_acc)\n",
    "    paired_curve.append(p_acc); unpaired_curve.append(u_acc)\n",
    "\n",
    "    epoch_index = EPOCHS_HEAD + ep\n",
    "    print(f\"[FINE] Epoch {epoch_index}/{EPOCHS_HEAD+EPOCHS_FINE} | Loss {t_loss:.3f} | Val Acc {v_acc:.2f}% | Unpaired {u_acc:.2f}%\")\n",
    "\n",
    "    if v_acc > best_overall:\n",
    "        best_overall = v_acc\n",
    "        torch.save({\n",
    "            \"epoch\": epoch_index,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"val_top1\": v_acc,\n",
    "            \"unpaired_top1\": u_acc\n",
    "        }, best_overall_path)\n",
    "        print(\"ðŸ’¾ Saved best_overall\")\n",
    "        \n",
    "    if u_acc > best_unpaired:\n",
    "        best_unpaired = u_acc\n",
    "        torch.save({\n",
    "            \"epoch\": epoch_index,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"val_top1\": v_acc,\n",
    "            \"unpaired_top1\": u_acc\n",
    "        }, best_unpaired_path)\n",
    "        print(\"ðŸ’¾ Saved best_unpaired\")\n",
    "\n",
    "# --- Final plots ---\n",
    "if len(train_losses) > 0:\n",
    "    plt.figure(figsize=(14,5))\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.plot(train_losses, label=\"Train Loss\")\n",
    "    plt.plot(val_losses, label=\"Val Loss\")\n",
    "    plt.title(\"Loss\"); plt.legend(); plt.grid()\n",
    "\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.plot(train_accs, label=\"Train Top1\")\n",
    "    plt.plot(val_accs, label=\"Val Top1\")\n",
    "    plt.title(\"Top-1 Acc\"); plt.legend(); plt.grid()\n",
    "\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.plot(paired_curve, label=\"Paired Top1\")\n",
    "    plt.plot(unpaired_curve, label=\"Unpaired Top1\")\n",
    "    plt.title(\"Paired / Unpaired\"); plt.legend(); plt.grid()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Done. Best overall:\", best_overall, \" Best unpaired:\", best_unpaired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 421241,
     "status": "aborted",
     "timestamp": 1764140421170,
     "user": {
      "displayName": "Esther Chai",
      "userId": "05451503381207659663"
     },
     "user_tz": -480
    },
    "id": "3N64TA6eITyN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš–ï¸  Loading Model:     c:\\Users\\William\\School\\Swinburne\\Computer Science\\2025 Semester 2\\COS30082 Applied Machine Learning\\Assignment 2\\Esther\\models\\best_unpaired.pth\n",
      "âœ… Model weights loaded successfully.\n",
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52/52 [00:11<00:00,  4.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== FINAL EVALUATION =====\n",
      "Overall Top-1 : 72.95%\n",
      "Overall Top-5 : 84.54%\n",
      "------------------------------\n",
      "Paired Top-1  : 86.27%\n",
      "Paired Top-5  : 98.04%\n",
      "------------------------------\n",
      "Unpaired Top-1: 35.19%\n",
      "Unpaired Top-5: 46.30%\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ===== Block 14: FINAL EVALUATION=====\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "MODEL_PATH = SAVE_DIR / \"best_unpaired.pth\"\n",
    "\n",
    "print(f\"âš–ï¸  Loading Model:     {MODEL_PATH}\")\n",
    "\n",
    "if MODEL_PATH.exists():\n",
    "    checkpoint = torch.load(MODEL_PATH, map_location=device)\n",
    "    # Load the state dictionary\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "    print(\"âœ… Model weights loaded successfully.\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Could not find model at {MODEL_PATH}\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "total = 0\n",
    "top1 = 0\n",
    "top5 = 0\n",
    "\n",
    "paired_correct_1 = paired_correct_5 = 0\n",
    "paired_total = 0\n",
    "\n",
    "unpaired_correct_1 = unpaired_correct_5 = 0\n",
    "unpaired_total = 0\n",
    "\n",
    "print(\"Running evaluation...\")\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader, desc=\"Eval\"):\n",
    "        if batch is None:\n",
    "            continue\n",
    "\n",
    "        # val_loader returns imgs, labels, domain\n",
    "        imgs, labels, dom = batch\n",
    "        imgs = imgs.to(device); labels = labels.to(device)\n",
    "\n",
    "        logits = model(imgs)              # (B, 100)\n",
    "        preds_top1 = logits.argmax(dim=1) # (B)\n",
    "\n",
    "        # top-5 indices\n",
    "        _, preds_top5 = logits.topk(5, dim=1)  # (B, 5)\n",
    "\n",
    "        bs = labels.size(0)\n",
    "        total += bs\n",
    "\n",
    "        # ----- OVERALL TOP-1 -----\n",
    "        top1 += (preds_top1 == labels).sum().item()\n",
    "\n",
    "        # ----- OVERALL TOP-5 -----\n",
    "        for i in range(bs):\n",
    "            if labels[i] in preds_top5[i]:\n",
    "                top5 += 1\n",
    "\n",
    "        # ----- PAIRED / UNPAIRED TOP-1 / TOP-5 -----\n",
    "        for i in range(bs):\n",
    "            lab = int(labels[i].item())\n",
    "            pred1 = preds_top1[i]\n",
    "            pred5 = preds_top5[i]\n",
    "\n",
    "            if lab in unpaired_new_ids:\n",
    "                unpaired_total += 1\n",
    "\n",
    "                # Top-1\n",
    "                if pred1 == labels[i]:\n",
    "                    unpaired_correct_1 += 1\n",
    "\n",
    "                # Top-5\n",
    "                if labels[i] in pred5:\n",
    "                    unpaired_correct_5 += 1\n",
    "\n",
    "            else:\n",
    "                paired_total += 1\n",
    "\n",
    "                # Top-1\n",
    "                if pred1 == labels[i]:\n",
    "                    paired_correct_1 += 1\n",
    "\n",
    "                # Top-5\n",
    "                if labels[i] in pred5:\n",
    "                    paired_correct_5 += 1\n",
    "\n",
    "# --- PRINT RESULTS ---\n",
    "print(\"\\n===== FINAL EVALUATION =====\")\n",
    "print(f\"Overall Top-1 : {100.0 * top1 / max(1, total):.2f}%\")\n",
    "print(f\"Overall Top-5 : {100.0 * top5 / max(1, total):.2f}%\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Paired Top-1  : {100.0 * paired_correct_1 / max(1, paired_total):.2f}%\")\n",
    "print(f\"Paired Top-5  : {100.0 * paired_correct_5 / max(1, paired_total):.2f}%\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Unpaired Top-1: {100.0 * unpaired_correct_1 / max(1, unpaired_total):.2f}%\")\n",
    "print(f\"Unpaired Top-5: {100.0 * unpaired_correct_5 / max(1, unpaired_total):.2f}%\")\n",
    "print(\"=\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 421242,
     "status": "aborted",
     "timestamp": 1764140421172,
     "user": {
      "displayName": "Esther Chai",
      "userId": "05451503381207659663"
     },
     "user_tz": -480
    },
    "id": "foEk4N-VIX7O"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved final checkpoint: c:\\Users\\William\\School\\Swinburne\\Computer Science\\2025 Semester 2\\COS30082 Applied Machine Learning\\Assignment 2\\Esther\\models\\final_finetuned_with_synth.pth.tar\n"
     ]
    }
   ],
   "source": [
    "# Block 15 - save final weights & metadata\n",
    "out_path = Path(PROJECT_ROOT) / \"models\" / \"final_finetuned_with_synth.pth.tar\"\n",
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"orig_to_new\": orig_to_new,\n",
    "    \"new_to_orig\": new_to_orig,\n",
    "    \"unpaired_orig_ids\": unpaired_orig_ids,\n",
    "    \"unpaired_new_ids\": sorted(list(unpaired_new_ids)),\n",
    "}, out_path)\n",
    "print(\"Saved final checkpoint:\", out_path)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMSNHJF/dgq/FVCToOFwndR",
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
